{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Pre-Processing and Training Data<a id='4_Pre-Processing_and_Training_Data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Contents<a id='4.1_Contents'></a>\n",
    "* [4 Pre-Processing and Training Data](#4_Pre-Processing_and_Training_Data)\n",
    "  * [4.1 Contents](#4.1_Contents)\n",
    "  * [4.2 Introduction](#4.2_Introduction)\n",
    "  * [4.3 Imports](#4.3_Imports)\n",
    "  * [4.4 Load Data](#4.4_Load_Data)\n",
    "  * [4.5 Extract Big Mountain Data](#4.5_Extract_Big_Mountain_Data)\n",
    "  * [4.6 Train/Test Split](#4.6_Train/Test_Split)\n",
    "  * [4.7 Initial Not-Even-A-Model](#4.7_Initial_Not-Even-A-Model)\n",
    "    * [4.7.1 Metrics](#4.7.1_Metrics)\n",
    "      * [4.7.1.1 R-squared, or coefficient of determination](#4.7.1.1_R-squared,_or_coefficient_of_determination)\n",
    "      * [4.7.1.2 Mean Absolute Error](#4.7.1.2_Mean_Absolute_Error)\n",
    "      * [4.7.1.3 Mean Squared Error](#4.7.1.3_Mean_Squared_Error)\n",
    "    * [4.7.2 sklearn metrics](#4.7.2_sklearn_metrics)\n",
    "        * [4.7.2.0.1 R-squared](#4.7.2.0.1_R-squared)\n",
    "        * [4.7.2.0.2 Mean absolute error](#4.7.2.0.2_Mean_absolute_error)\n",
    "        * [4.7.2.0.3 Mean squared error](#4.7.2.0.3_Mean_squared_error)\n",
    "    * [4.7.3 Note On Calculating Metrics](#4.7.3_Note_On_Calculating_Metrics)\n",
    "  * [4.8 Initial Models](#4.8_Initial_Models)\n",
    "    * [4.8.1 Imputing missing feature (predictor) values](#4.8.1_Imputing_missing_feature_(predictor)_values)\n",
    "      * [4.8.1.1 Impute missing values with median](#4.8.1.1_Impute_missing_values_with_median)\n",
    "        * [4.8.1.1.1 Learn the values to impute from the train set](#4.8.1.1.1_Learn_the_values_to_impute_from_the_train_set)\n",
    "        * [4.8.1.1.2 Apply the imputation to both train and test splits](#4.8.1.1.2_Apply_the_imputation_to_both_train_and_test_splits)\n",
    "        * [4.8.1.1.3 Scale the data](#4.8.1.1.3_Scale_the_data)\n",
    "        * [4.8.1.1.4 Train the model on the train split](#4.8.1.1.4_Train_the_model_on_the_train_split)\n",
    "        * [4.8.1.1.5 Make predictions using the model on both train and test splits](#4.8.1.1.5_Make_predictions_using_the_model_on_both_train_and_test_splits)\n",
    "        * [4.8.1.1.6 Assess model performance](#4.8.1.1.6_Assess_model_performance)\n",
    "      * [4.8.1.2 Impute missing values with the mean](#4.8.1.2_Impute_missing_values_with_the_mean)\n",
    "        * [4.8.1.2.1 Learn the values to impute from the train set](#4.8.1.2.1_Learn_the_values_to_impute_from_the_train_set)\n",
    "        * [4.8.1.2.2 Apply the imputation to both train and test splits](#4.8.1.2.2_Apply_the_imputation_to_both_train_and_test_splits)\n",
    "        * [4.8.1.2.3 Scale the data](#4.8.1.2.3_Scale_the_data)\n",
    "        * [4.8.1.2.4 Train the model on the train split](#4.8.1.2.4_Train_the_model_on_the_train_split)\n",
    "        * [4.8.1.2.5 Make predictions using the model on both train and test splits](#4.8.1.2.5_Make_predictions_using_the_model_on_both_train_and_test_splits)\n",
    "        * [4.8.1.2.6 Assess model performance](#4.8.1.2.6_Assess_model_performance)\n",
    "    * [4.8.2 Pipelines](#4.8.2_Pipelines)\n",
    "      * [4.8.2.1 Define the pipeline](#4.8.2.1_Define_the_pipeline)\n",
    "      * [4.8.2.2 Fit the pipeline](#4.8.2.2_Fit_the_pipeline)\n",
    "      * [4.8.2.3 Make predictions on the train and test sets](#4.8.2.3_Make_predictions_on_the_train_and_test_sets)\n",
    "      * [4.8.2.4 Assess performance](#4.8.2.4_Assess_performance)\n",
    "  * [4.9 Refining The Linear Model](#4.9_Refining_The_Linear_Model)\n",
    "    * [4.9.1 Define the pipeline](#4.9.1_Define_the_pipeline)\n",
    "    * [4.9.2 Fit the pipeline](#4.9.2_Fit_the_pipeline)\n",
    "    * [4.9.3 Assess performance on the train and test set](#4.9.3_Assess_performance_on_the_train_and_test_set)\n",
    "    * [4.9.4 Define a new pipeline to select a different number of features](#4.9.4_Define_a_new_pipeline_to_select_a_different_number_of_features)\n",
    "    * [4.9.5 Fit the pipeline](#4.9.5_Fit_the_pipeline)\n",
    "    * [4.9.6 Assess performance on train and test data](#4.9.6_Assess_performance_on_train_and_test_data)\n",
    "    * [4.9.7 Assessing performance using cross-validation](#4.9.7_Assessing_performance_using_cross-validation)\n",
    "    * [4.9.8 Hyperparameter search using GridSearchCV](#4.9.8_Hyperparameter_search_using_GridSearchCV)\n",
    "  * [4.10 Random Forest Model](#4.10_Random_Forest_Model)\n",
    "    * [4.10.1 Define the pipeline](#4.10.1_Define_the_pipeline)\n",
    "    * [4.10.2 Fit and assess performance using cross-validation](#4.10.2_Fit_and_assess_performance_using_cross-validation)\n",
    "    * [4.10.3 Hyperparameter search using GridSearchCV](#4.10.3_Hyperparameter_search_using_GridSearchCV)\n",
    "  * [4.11 Final Model Selection](#4.11_Final_Model_Selection)\n",
    "    * [4.11.1 Linear regression model performance](#4.11.1_Linear_regression_model_performance)\n",
    "    * [4.11.2 Random forest regression model performance](#4.11.2_Random_forest_regression_model_performance)\n",
    "    * [4.11.3 Conclusion](#4.11.3_Conclusion)\n",
    "  * [4.12 Data quantity assessment](#4.12_Data_quantity_assessment)\n",
    "  * [4.13 Save best model object from pipeline](#4.13_Save_best_model_object_from_pipeline)\n",
    "  * [4.14 Summary](#4.14_Summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Introduction<a id='4.2_Introduction'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preceding notebooks, performed preliminary assessments of data quality and refined the question to be answered. You found a small number of data values that gave clear choices about whether to replace values or drop a whole row. You determined that predicting the adult weekend ticket price was your primary aim. You threw away records with missing price data, but not before making the most of the other available data to look for any patterns between the states. You didn't see any and decided to treat all states equally; the state label didn't seem to be particularly useful.\n",
    "\n",
    "In this notebook you'll start to build machine learning models. Before even starting with learning a machine learning model, however, start by considering how useful the mean value is as a predictor. This is more than just a pedagogical device. You never want to go to stakeholders with a machine learning model only to have the CEO point out that it performs worse than just guessing the average! Your first model is a baseline performance comparitor for any subsequent model. You then build up the process of efficiently and robustly creating and assessing models against it. The development we lay out may be little slower than in the real world, but this step of the capstone is definitely more than just instructional. It is good practice to build up an understanding that the machine learning pipelines you build work as expected. You can validate steps with your own functions for checking expected equivalence between, say, pandas and sklearn implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#That's right, we analyzed the data utilizing the data we still had even in the rows that had missing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Imports<a id='4.3_Imports'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import __version__ as sklearn_version\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import datetime\n",
    "\n",
    "from library.sb_utils import save_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Load Data<a id='4.4_Load_Data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <td>Alyeska Resort</td>\n",
       "      <td>Eaglecrest Ski Area</td>\n",
       "      <td>Hilltop Ski Area</td>\n",
       "      <td>Arizona Snowbowl</td>\n",
       "      <td>Sunrise Park Resort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Region</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summit_elev</th>\n",
       "      <td>3939</td>\n",
       "      <td>2600</td>\n",
       "      <td>2090</td>\n",
       "      <td>11500</td>\n",
       "      <td>11100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vertical_drop</th>\n",
       "      <td>2500</td>\n",
       "      <td>1540</td>\n",
       "      <td>294</td>\n",
       "      <td>2300</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_elev</th>\n",
       "      <td>250</td>\n",
       "      <td>1200</td>\n",
       "      <td>1796</td>\n",
       "      <td>9200</td>\n",
       "      <td>9200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trams</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastSixes</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastQuads</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quad</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>triple</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>double</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surface</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_chairs</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Runs</th>\n",
       "      <td>76.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TerrainParks</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LongestRun_mi</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SkiableTerrain_ac</th>\n",
       "      <td>1610.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>777.0</td>\n",
       "      <td>800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Snow Making_ac</th>\n",
       "      <td>113.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daysOpenLastYear</th>\n",
       "      <td>150.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yearsOpen</th>\n",
       "      <td>60.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>averageSnowfall</th>\n",
       "      <td>669.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdultWeekend</th>\n",
       "      <td>85.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>projectedDaysOpen</th>\n",
       "      <td>150.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NightSkiing_ac</th>\n",
       "      <td>550.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total_Resorts</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resorts_per_100kcapita</th>\n",
       "      <td>0.410091</td>\n",
       "      <td>0.410091</td>\n",
       "      <td>0.410091</td>\n",
       "      <td>0.027477</td>\n",
       "      <td>0.027477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resorts_per_100ksq_mile</th>\n",
       "      <td>0.450867</td>\n",
       "      <td>0.450867</td>\n",
       "      <td>0.450867</td>\n",
       "      <td>1.75454</td>\n",
       "      <td>1.75454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resort_skiable_area_ac_state_ratio</th>\n",
       "      <td>0.70614</td>\n",
       "      <td>0.280702</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.492708</td>\n",
       "      <td>0.507292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resort_days_open_state_ratio</th>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.514768</td>\n",
       "      <td>0.485232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resort_terrain_park_state_ratio</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resort_night_skiing_state_ratio</th>\n",
       "      <td>0.948276</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_chairs_runs_ratio</th>\n",
       "      <td>0.092105</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.107692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_chairs_skiable_ratio</th>\n",
       "      <td>0.004348</td>\n",
       "      <td>0.00625</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.010296</td>\n",
       "      <td>0.00875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastQuads_runs_ratio</th>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastQuads_skiable_ratio</th>\n",
       "      <td>0.001242</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 0                    1  \\\n",
       "Name                                Alyeska Resort  Eaglecrest Ski Area   \n",
       "Region                                      Alaska               Alaska   \n",
       "state                                       Alaska               Alaska   \n",
       "summit_elev                                   3939                 2600   \n",
       "vertical_drop                                 2500                 1540   \n",
       "base_elev                                      250                 1200   \n",
       "trams                                            1                    0   \n",
       "fastSixes                                        0                    0   \n",
       "fastQuads                                        2                    0   \n",
       "quad                                             2                    0   \n",
       "triple                                           0                    0   \n",
       "double                                           0                    4   \n",
       "surface                                          2                    0   \n",
       "total_chairs                                     7                    4   \n",
       "Runs                                          76.0                 36.0   \n",
       "TerrainParks                                   2.0                  1.0   \n",
       "LongestRun_mi                                  1.0                  2.0   \n",
       "SkiableTerrain_ac                           1610.0                640.0   \n",
       "Snow Making_ac                               113.0                 60.0   \n",
       "daysOpenLastYear                             150.0                 45.0   \n",
       "yearsOpen                                     60.0                 44.0   \n",
       "averageSnowfall                              669.0                350.0   \n",
       "AdultWeekend                                  85.0                 53.0   \n",
       "projectedDaysOpen                            150.0                 90.0   \n",
       "NightSkiing_ac                               550.0                  NaN   \n",
       "Total_Resorts                                    3                    3   \n",
       "resorts_per_100kcapita                    0.410091             0.410091   \n",
       "resorts_per_100ksq_mile                   0.450867             0.450867   \n",
       "resort_skiable_area_ac_state_ratio         0.70614             0.280702   \n",
       "resort_days_open_state_ratio              0.434783             0.130435   \n",
       "resort_terrain_park_state_ratio                0.5                 0.25   \n",
       "resort_night_skiing_state_ratio           0.948276                  NaN   \n",
       "total_chairs_runs_ratio                   0.092105             0.111111   \n",
       "total_chairs_skiable_ratio                0.004348              0.00625   \n",
       "fastQuads_runs_ratio                      0.026316                  0.0   \n",
       "fastQuads_skiable_ratio                   0.001242                  0.0   \n",
       "\n",
       "                                                   2                 3  \\\n",
       "Name                                Hilltop Ski Area  Arizona Snowbowl   \n",
       "Region                                        Alaska           Arizona   \n",
       "state                                         Alaska           Arizona   \n",
       "summit_elev                                     2090             11500   \n",
       "vertical_drop                                    294              2300   \n",
       "base_elev                                       1796              9200   \n",
       "trams                                              0                 0   \n",
       "fastSixes                                          0                 1   \n",
       "fastQuads                                          0                 0   \n",
       "quad                                               0                 2   \n",
       "triple                                             1                 2   \n",
       "double                                             0                 1   \n",
       "surface                                            2                 2   \n",
       "total_chairs                                       3                 8   \n",
       "Runs                                            13.0              55.0   \n",
       "TerrainParks                                     1.0               4.0   \n",
       "LongestRun_mi                                    1.0               2.0   \n",
       "SkiableTerrain_ac                               30.0             777.0   \n",
       "Snow Making_ac                                  30.0             104.0   \n",
       "daysOpenLastYear                               150.0             122.0   \n",
       "yearsOpen                                       36.0              81.0   \n",
       "averageSnowfall                                 69.0             260.0   \n",
       "AdultWeekend                                    34.0              89.0   \n",
       "projectedDaysOpen                              152.0             122.0   \n",
       "NightSkiing_ac                                  30.0               NaN   \n",
       "Total_Resorts                                      3                 2   \n",
       "resorts_per_100kcapita                      0.410091          0.027477   \n",
       "resorts_per_100ksq_mile                     0.450867           1.75454   \n",
       "resort_skiable_area_ac_state_ratio          0.013158          0.492708   \n",
       "resort_days_open_state_ratio                0.434783          0.514768   \n",
       "resort_terrain_park_state_ratio                 0.25          0.666667   \n",
       "resort_night_skiing_state_ratio             0.051724               NaN   \n",
       "total_chairs_runs_ratio                     0.230769          0.145455   \n",
       "total_chairs_skiable_ratio                       0.1          0.010296   \n",
       "fastQuads_runs_ratio                             0.0               0.0   \n",
       "fastQuads_skiable_ratio                          0.0               0.0   \n",
       "\n",
       "                                                      4  \n",
       "Name                                Sunrise Park Resort  \n",
       "Region                                          Arizona  \n",
       "state                                           Arizona  \n",
       "summit_elev                                       11100  \n",
       "vertical_drop                                      1800  \n",
       "base_elev                                          9200  \n",
       "trams                                                 0  \n",
       "fastSixes                                             0  \n",
       "fastQuads                                             1  \n",
       "quad                                                  2  \n",
       "triple                                                3  \n",
       "double                                                1  \n",
       "surface                                               0  \n",
       "total_chairs                                          7  \n",
       "Runs                                               65.0  \n",
       "TerrainParks                                        2.0  \n",
       "LongestRun_mi                                       1.2  \n",
       "SkiableTerrain_ac                                 800.0  \n",
       "Snow Making_ac                                     80.0  \n",
       "daysOpenLastYear                                  115.0  \n",
       "yearsOpen                                          49.0  \n",
       "averageSnowfall                                   250.0  \n",
       "AdultWeekend                                       78.0  \n",
       "projectedDaysOpen                                 104.0  \n",
       "NightSkiing_ac                                     80.0  \n",
       "Total_Resorts                                         2  \n",
       "resorts_per_100kcapita                         0.027477  \n",
       "resorts_per_100ksq_mile                         1.75454  \n",
       "resort_skiable_area_ac_state_ratio             0.507292  \n",
       "resort_days_open_state_ratio                   0.485232  \n",
       "resort_terrain_park_state_ratio                0.333333  \n",
       "resort_night_skiing_state_ratio                     1.0  \n",
       "total_chairs_runs_ratio                        0.107692  \n",
       "total_chairs_skiable_ratio                      0.00875  \n",
       "fastQuads_runs_ratio                           0.015385  \n",
       "fastQuads_skiable_ratio                         0.00125  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ski_data = pd.read_csv('../data/ski_data_step3_features.csv')\n",
    "ski_data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm wondering if my version is the same as what they intended. I mean the main thing is adding the several features\n",
    "#But I don't know which all rows and columns they took out...\n",
    "#I'm not sure if they already had a file in place there\n",
    "#But if so I should have kept a separate file of mine rather than overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328, 36)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ski_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Extract Big Mountain Data<a id='4.5_Extract_Big_Mountain_Data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big Mountain is your resort. Separate it from the rest of the data to use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_mountain = ski_data[ski_data.Name == 'Big Mountain Resort']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>150</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <td>Big Mountain Resort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Region</th>\n",
       "      <td>Montana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>Montana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summit_elev</th>\n",
       "      <td>6817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vertical_drop</th>\n",
       "      <td>2353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_elev</th>\n",
       "      <td>4464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trams</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastSixes</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastQuads</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quad</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>triple</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>double</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surface</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_chairs</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Runs</th>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TerrainParks</th>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LongestRun_mi</th>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SkiableTerrain_ac</th>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Snow Making_ac</th>\n",
       "      <td>600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daysOpenLastYear</th>\n",
       "      <td>123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yearsOpen</th>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>averageSnowfall</th>\n",
       "      <td>333.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdultWeekend</th>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>projectedDaysOpen</th>\n",
       "      <td>123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NightSkiing_ac</th>\n",
       "      <td>600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total_Resorts</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resorts_per_100kcapita</th>\n",
       "      <td>1.122778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resorts_per_100ksq_mile</th>\n",
       "      <td>8.161045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resort_skiable_area_ac_state_ratio</th>\n",
       "      <td>0.140121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resort_days_open_state_ratio</th>\n",
       "      <td>0.129338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resort_terrain_park_state_ratio</th>\n",
       "      <td>0.148148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resort_night_skiing_state_ratio</th>\n",
       "      <td>0.84507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_chairs_runs_ratio</th>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_chairs_skiable_ratio</th>\n",
       "      <td>0.004667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastQuads_runs_ratio</th>\n",
       "      <td>0.028571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastQuads_skiable_ratio</th>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    150\n",
       "Name                                Big Mountain Resort\n",
       "Region                                          Montana\n",
       "state                                           Montana\n",
       "summit_elev                                        6817\n",
       "vertical_drop                                      2353\n",
       "base_elev                                          4464\n",
       "trams                                                 0\n",
       "fastSixes                                             0\n",
       "fastQuads                                             3\n",
       "quad                                                  2\n",
       "triple                                                6\n",
       "double                                                0\n",
       "surface                                               3\n",
       "total_chairs                                         14\n",
       "Runs                                              105.0\n",
       "TerrainParks                                        4.0\n",
       "LongestRun_mi                                       3.3\n",
       "SkiableTerrain_ac                                3000.0\n",
       "Snow Making_ac                                    600.0\n",
       "daysOpenLastYear                                  123.0\n",
       "yearsOpen                                          72.0\n",
       "averageSnowfall                                   333.0\n",
       "AdultWeekend                                       81.0\n",
       "projectedDaysOpen                                 123.0\n",
       "NightSkiing_ac                                    600.0\n",
       "Total_Resorts                                        12\n",
       "resorts_per_100kcapita                         1.122778\n",
       "resorts_per_100ksq_mile                        8.161045\n",
       "resort_skiable_area_ac_state_ratio             0.140121\n",
       "resort_days_open_state_ratio                   0.129338\n",
       "resort_terrain_park_state_ratio                0.148148\n",
       "resort_night_skiing_state_ratio                 0.84507\n",
       "total_chairs_runs_ratio                        0.133333\n",
       "total_chairs_skiable_ratio                     0.004667\n",
       "fastQuads_runs_ratio                           0.028571\n",
       "fastQuads_skiable_ratio                           0.001"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_mountain.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hmmm so for me it's showing as 150, but for them it's 124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328, 36)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ski_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ahh okay, this is why. they only have 277 rows. they're def using the trimmed dataset\n",
    "#let me work on that:\n",
    "\n",
    "ski_data = ski_data[ski_data.yearsOpen < 1000]\n",
    "\n",
    "\n",
    "#ahhh so i totally forgot this in the original version of this! could this be that this is why the scores were\n",
    "#messed up for my test run\n",
    "ski_data.loc[ski_data.Name=='Silverton Mountain', 'SkiableTerrain_ac'] = 1819\n",
    "\n",
    "\n",
    "######**********ALHAMDULILLAH YES!!!!!! THAT'S EXACTLY WHAT IT WAS!!!!***********#########\n",
    "#It's because i didn't do this edit from its outrageously high original value, like nearly 27,000!!!!\n",
    "#and then kept getting thrown in w/ / batched w/ the TEST set and throwing off everything / skewing the values\n",
    "#and effectiveness of the prediction model way way off cuz the model wasn't prepared to handle such crazy outliers!!!!\n",
    "\n",
    "\n",
    "# missing_price = ski_data[['AdultWeekend', 'AdultWeekday']].isnull().sum(axis=1)\n",
    "\n",
    "# ski_data = ski_data[missing_price != 2]\n",
    "\n",
    "ski_data.dropna(subset=['AdultWeekend'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(277, 36)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ski_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok aH theek hai should be fixed now aH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ski_data = ski_data[ski_data.Name != 'Big Mountain Resort']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hain? but why do we wanna get rid of it and nix it off altogether?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(276, 36)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ski_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name                                    0\n",
       "Region                                  0\n",
       "state                                   0\n",
       "summit_elev                             0\n",
       "vertical_drop                           0\n",
       "base_elev                               0\n",
       "trams                                   0\n",
       "fastSixes                               0\n",
       "fastQuads                               0\n",
       "quad                                    0\n",
       "triple                                  0\n",
       "double                                  0\n",
       "surface                                 0\n",
       "total_chairs                            0\n",
       "Runs                                    3\n",
       "TerrainParks                           44\n",
       "LongestRun_mi                           5\n",
       "SkiableTerrain_ac                       2\n",
       "Snow Making_ac                         37\n",
       "daysOpenLastYear                       44\n",
       "yearsOpen                               0\n",
       "averageSnowfall                         9\n",
       "AdultWeekend                            0\n",
       "projectedDaysOpen                      41\n",
       "NightSkiing_ac                        114\n",
       "Total_Resorts                           0\n",
       "resorts_per_100kcapita                  0\n",
       "resorts_per_100ksq_mile                 0\n",
       "resort_skiable_area_ac_state_ratio      2\n",
       "resort_days_open_state_ratio           44\n",
       "resort_terrain_park_state_ratio        44\n",
       "resort_night_skiing_state_ratio       114\n",
       "total_chairs_runs_ratio                 3\n",
       "total_chairs_skiable_ratio              2\n",
       "fastQuads_runs_ratio                    3\n",
       "fastQuads_skiable_ratio                 2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ski_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not sure how this compares to the actual one they use... should be the same tho?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Train/Test Split<a id='4.6_Train/Test_Split'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, you've treated ski resort data as a single entity. In machine learning, when you train your model on all of your data, you end up with no data set aside to evaluate model performance. You could keep making more and more complex models that fit the data better and better and not realise you were overfitting to that one set of samples. By partitioning the data into training and testing splits, without letting a model (or missing-value imputation) learn anything about the test split, you have a somewhat independent assessment of how your model might perform in the future. An often overlooked subtlety here is that people all too frequently use the test set to assess model performance _and then compare multiple models to pick the best_. This means their overall model selection process is  fitting to one specific data set, now the test split. You could keep going, trying to get better and better performance on that one data set, but that's  where cross-validation becomes especially useful. While training models, a test split is very useful as a final check on expected future performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ahh yeah makes sense. since we have no other data to test out on, WE CAN'T USE OUR ENTIRE DATASET TO FORM THE MODEL /\n",
    "#TRAIN!!!\n",
    "\n",
    "#and yes, the pitfall run into in dasci is coming up with a model TOO specific to JUST your one dataset!\n",
    "\n",
    "#so we're gonna split our data to leave some for testing\n",
    "\n",
    "#we're kind of doing a trick/hack - so we already used ALL our data to train / fit. but now we're gonna shear off a\n",
    "#portion of the data to test on and simply see if it lines up!\n",
    "\n",
    "#{trying to paraphrase what they're saying above}:\n",
    "#\"another pitfall ppl fall into is testing a model on the test portion and then testing multiple models to see which\n",
    "#is best. But the problem is that this is effectively picking a model based off its fit to the TEST data, NOT the\n",
    "#training data. So it's still the same pitfall as above, just relocated - fitting too specifically to just one set\n",
    "\n",
    "#then ppl get over obsessed w/ trying to make the model fit the new test set, but again - the problem w/ that is then\n",
    "#it's just learning ONE SPECIFIC TYPE OF DATA!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What partition sizes would you have with a 70/30 train/test split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(193.2, 82.8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#meaning splitting our 276 NON-BIG-MOUNTAIN ROWS into 70-training/30-test\n",
    "len(ski_data) * .7, len(ski_data) * .3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ahhh okay, so we split off our Big Mountain resort so that we can specifically test on / predict on THAT ALONE!\n",
    "#but isn't the point that we're tryna SET an optimal price? NOT that we're trying to PREDICT it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Added this to make it easier to understand:\n",
    "X = ski_data.drop(columns='AdultWeekend')\n",
    "y = ski_data.AdultWeekend\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=47)\n",
    "\n",
    "#okay what is this saying/doing?\n",
    "#okay so train_test_split IS actually a sickit learn preloaded/predefined function:\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "#so this is saying that / what this is doing is: we're splitting the data first of all into 70/30 train/test,\n",
    "#and so X_train & y_train will get 70% of their respective data and the balance 30% will go to X_test, y_test!\n",
    "#so that's why we specify those 4 variables at the outset\n",
    "#the X is everything BUT the price since that's what we wanna predict BASED ON IT i.e. the y!!!\n",
    "#and thus the y is JUST the price column\n",
    "\n",
    "#so we feed it these two arrays, and then it's gonna split it / cut it up accordingly\n",
    "\n",
    "#so the formula/function knows to split the data according to how we specify for train/test for both X & y\n",
    "\n",
    "\n",
    "#'X_train' is our latest reduced but expanded overall dataset, MINUS the weekend price column altogether -\n",
    "#THAT'S BECAUSE THAT'S WHAT WE'RE TRYING TO PREDICT / THAT'S GONNA BE THE Y!!! X_train is 70% of (the) X\n",
    "\n",
    "#X_test is the remaining/remainder 30% of X\n",
    "\n",
    "\n",
    "#and thus y_train & y_test are the random 70/30 split of the y data i.e. the price data\n",
    "\n",
    "\n",
    "#test size is telling it the PORTION of the loaded data that will be for testing, vs training\n",
    "\n",
    "#random state tells it how much to shuffle? and something about that this is like a unique identifier that will make\n",
    "#the random results reproducible if we wanna run it again? i remember we dealt w/ something about that, but don't know\n",
    "#if it was random_state - not finding that in my notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((193, 35), (83, 35))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((193,), (83,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of 133     48.0\n",
       "120     50.0\n",
       "218     68.0\n",
       "269    119.0\n",
       "1       53.0\n",
       "       ...  \n",
       "37      89.0\n",
       "209     55.0\n",
       "94      71.0\n",
       "314     47.0\n",
       "163     58.0\n",
       "Name: AdultWeekend, Length: 193, dtype: float64>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#note the notation when there's only one column... for some reason it won't say '1'\n",
    "#let's see what it looks like just to confirm...\n",
    "\n",
    "y_train.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Region</th>\n",
       "      <th>state</th>\n",
       "      <th>summit_elev</th>\n",
       "      <th>vertical_drop</th>\n",
       "      <th>base_elev</th>\n",
       "      <th>trams</th>\n",
       "      <th>fastSixes</th>\n",
       "      <th>fastQuads</th>\n",
       "      <th>quad</th>\n",
       "      <th>...</th>\n",
       "      <th>resorts_per_100kcapita</th>\n",
       "      <th>resorts_per_100ksq_mile</th>\n",
       "      <th>resort_skiable_area_ac_state_ratio</th>\n",
       "      <th>resort_days_open_state_ratio</th>\n",
       "      <th>resort_terrain_park_state_ratio</th>\n",
       "      <th>resort_night_skiing_state_ratio</th>\n",
       "      <th>total_chairs_runs_ratio</th>\n",
       "      <th>total_chairs_skiable_ratio</th>\n",
       "      <th>fastQuads_runs_ratio</th>\n",
       "      <th>fastQuads_skiable_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alyeska Resort</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>3939</td>\n",
       "      <td>2500</td>\n",
       "      <td>250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410091</td>\n",
       "      <td>0.450867</td>\n",
       "      <td>0.706140</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.092105</td>\n",
       "      <td>0.004348</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.001242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eaglecrest Ski Area</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>2600</td>\n",
       "      <td>1540</td>\n",
       "      <td>1200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410091</td>\n",
       "      <td>0.450867</td>\n",
       "      <td>0.280702</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hilltop Ski Area</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>2090</td>\n",
       "      <td>294</td>\n",
       "      <td>1796</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410091</td>\n",
       "      <td>0.450867</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arizona Snowbowl</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>11500</td>\n",
       "      <td>2300</td>\n",
       "      <td>9200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027477</td>\n",
       "      <td>1.754540</td>\n",
       "      <td>0.492708</td>\n",
       "      <td>0.514768</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.010296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sunrise Park Resort</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>11100</td>\n",
       "      <td>1800</td>\n",
       "      <td>9200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027477</td>\n",
       "      <td>1.754540</td>\n",
       "      <td>0.507292</td>\n",
       "      <td>0.485232</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.008750</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>0.001250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Name   Region    state  summit_elev  vertical_drop  \\\n",
       "0       Alyeska Resort   Alaska   Alaska         3939           2500   \n",
       "1  Eaglecrest Ski Area   Alaska   Alaska         2600           1540   \n",
       "2     Hilltop Ski Area   Alaska   Alaska         2090            294   \n",
       "3     Arizona Snowbowl  Arizona  Arizona        11500           2300   \n",
       "4  Sunrise Park Resort  Arizona  Arizona        11100           1800   \n",
       "\n",
       "   base_elev  trams  fastSixes  fastQuads  quad  ...  resorts_per_100kcapita  \\\n",
       "0        250      1          0          2     2  ...                0.410091   \n",
       "1       1200      0          0          0     0  ...                0.410091   \n",
       "2       1796      0          0          0     0  ...                0.410091   \n",
       "3       9200      0          1          0     2  ...                0.027477   \n",
       "4       9200      0          0          1     2  ...                0.027477   \n",
       "\n",
       "   resorts_per_100ksq_mile  resort_skiable_area_ac_state_ratio  \\\n",
       "0                 0.450867                            0.706140   \n",
       "1                 0.450867                            0.280702   \n",
       "2                 0.450867                            0.013158   \n",
       "3                 1.754540                            0.492708   \n",
       "4                 1.754540                            0.507292   \n",
       "\n",
       "   resort_days_open_state_ratio  resort_terrain_park_state_ratio  \\\n",
       "0                      0.434783                         0.500000   \n",
       "1                      0.130435                         0.250000   \n",
       "2                      0.434783                         0.250000   \n",
       "3                      0.514768                         0.666667   \n",
       "4                      0.485232                         0.333333   \n",
       "\n",
       "   resort_night_skiing_state_ratio  total_chairs_runs_ratio  \\\n",
       "0                         0.948276                 0.092105   \n",
       "1                              NaN                 0.111111   \n",
       "2                         0.051724                 0.230769   \n",
       "3                              NaN                 0.145455   \n",
       "4                         1.000000                 0.107692   \n",
       "\n",
       "   total_chairs_skiable_ratio  fastQuads_runs_ratio  fastQuads_skiable_ratio  \n",
       "0                    0.004348              0.026316                 0.001242  \n",
       "1                    0.006250              0.000000                 0.000000  \n",
       "2                    0.100000              0.000000                 0.000000  \n",
       "3                    0.010296              0.000000                 0.000000  \n",
       "4                    0.008750              0.015385                 0.001250  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ski_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ahh okay i just realized now that i look at this that i never reset the index for ski_data so it's a little messy\n",
    "#well, actually it doesn't matter\n",
    "#but also, THEY DIDN'T EITHER - so i don't wanna risk wandering / venturing off on my own again and then have little\n",
    "#mismatches now that lead to huge problems later and wild goose chases, don't wanna have to keep track of a parallel\n",
    "#set this whole time! but just for myself:\n",
    "# ski_data_sm = ski_data.copy()\n",
    "# ski_data_sm.reset_index(drop=True, inplace=True)\n",
    "# ski_data_sm.drop(columns=['level_0','index'],inplace=True)\n",
    "# ski_data_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay cool so i fixed my own personal copy. and JUST to make double sure & triple check that the original data wasn't\n",
    "#affected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ski_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#good aH we're in business then!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#had to go through this song and dance because i didn't save/checkpoint quite right and had to do exactly what i said\n",
    "#i wouldn't above about not messing around w/ the original data.... -----_________-----\n",
    "\n",
    "# ski_data_copy = ski_data.copy()\n",
    "# ski_data_copy.drop(columns='level_0', inplace=True)\n",
    "# ski_data_copy.reset_index(drop=True, inplace=True)\n",
    "# ski_data_copy.set_index('index', inplace=True)\n",
    "# ski_data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ski_data = ski_data_copy\n",
    "# ski_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((193, 32), (83, 32))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code task 1#\n",
    "#Save the 'Name', 'state', and 'Region' columns from the train/test data into names_train and names_test\n",
    "#Then drop those columns from `X_train` and `X_test`. Use 'inplace=True'\n",
    "\n",
    "#the columns that we will separate off from the rest of the data\n",
    "#storing as a variable list for easy access down\n",
    "names_list = ['Name', 'state', 'Region']\n",
    "\n",
    "#separate off these 3 & store in/as a new df\n",
    "names_train = X_train[names_list] #the resorts of the/Team TRAIN set!\n",
    "names_test = X_test[names_list] #the resorts of the/Team TEST set!\n",
    "#^^note that for/when you split a df into train/test you're only splitting the ROWS, *NOT* THE COLUMNS!!! SO YOU'LL\n",
    "#STILL HAVE ALL THE COLUMNS FOR EACH!!\n",
    "\n",
    "#okay so now that we've saved/stored these columns separately, we're gonna kick these off columns from\n",
    "#X_train & X_test\n",
    "\n",
    "X_train.drop(columns=names_list, inplace=True)\n",
    "X_test.drop(columns=names_list, inplace=True)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oops, i was supposed to use .copy() or like .iloc instead of straight 'chain-indexing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "summit_elev                             int64\n",
       "vertical_drop                           int64\n",
       "base_elev                               int64\n",
       "trams                                   int64\n",
       "fastSixes                               int64\n",
       "fastQuads                               int64\n",
       "quad                                    int64\n",
       "triple                                  int64\n",
       "double                                  int64\n",
       "surface                                 int64\n",
       "total_chairs                            int64\n",
       "Runs                                  float64\n",
       "TerrainParks                          float64\n",
       "LongestRun_mi                         float64\n",
       "SkiableTerrain_ac                     float64\n",
       "Snow Making_ac                        float64\n",
       "daysOpenLastYear                      float64\n",
       "yearsOpen                             float64\n",
       "averageSnowfall                       float64\n",
       "projectedDaysOpen                     float64\n",
       "NightSkiing_ac                        float64\n",
       "Total_Resorts                           int64\n",
       "resorts_per_100kcapita                float64\n",
       "resorts_per_100ksq_mile               float64\n",
       "resort_skiable_area_ac_state_ratio    float64\n",
       "resort_days_open_state_ratio          float64\n",
       "resort_terrain_park_state_ratio       float64\n",
       "resort_night_skiing_state_ratio       float64\n",
       "total_chairs_runs_ratio               float64\n",
       "total_chairs_skiable_ratio            float64\n",
       "fastQuads_runs_ratio                  float64\n",
       "fastQuads_skiable_ratio               float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code task 2#\n",
    "#Check the `dtypes` attribute of `X_train` to verify all features are numeric\n",
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "summit_elev                             int64\n",
       "vertical_drop                           int64\n",
       "base_elev                               int64\n",
       "trams                                   int64\n",
       "fastSixes                               int64\n",
       "fastQuads                               int64\n",
       "quad                                    int64\n",
       "triple                                  int64\n",
       "double                                  int64\n",
       "surface                                 int64\n",
       "total_chairs                            int64\n",
       "Runs                                  float64\n",
       "TerrainParks                          float64\n",
       "LongestRun_mi                         float64\n",
       "SkiableTerrain_ac                     float64\n",
       "Snow Making_ac                        float64\n",
       "daysOpenLastYear                      float64\n",
       "yearsOpen                             float64\n",
       "averageSnowfall                       float64\n",
       "projectedDaysOpen                     float64\n",
       "NightSkiing_ac                        float64\n",
       "Total_Resorts                           int64\n",
       "resorts_per_100kcapita                float64\n",
       "resorts_per_100ksq_mile               float64\n",
       "resort_skiable_area_ac_state_ratio    float64\n",
       "resort_days_open_state_ratio          float64\n",
       "resort_terrain_park_state_ratio       float64\n",
       "resort_night_skiing_state_ratio       float64\n",
       "total_chairs_runs_ratio               float64\n",
       "total_chairs_skiable_ratio            float64\n",
       "fastQuads_runs_ratio                  float64\n",
       "fastQuads_skiable_ratio               float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code task 3#\n",
    "#Repeat this check for the test split in `X_test`\n",
    "X_test.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have only numeric features in your X now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the two use the same columns so what was th point of checking both? assumably there weren't any mix of variables within\n",
    "#one column?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Initial Not-Even-A-Model<a id='4.7_Initial_Not-Even-A-Model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#^what's that supposed to mean? lol, now THAT'S funnier - 'mean,' get it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good place to start is to see how good the mean is as a predictor. In other words, what if you simply say your best guess is the average price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so you mean see how good / accurate it is to predict the price if we JUST / simply use the / go by/off the mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.811088082901556"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code task 4#\n",
    "#Calculate the mean of `y_train`\n",
    "train_mean = y_train.mean()\n",
    "train_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn`'s `DummyRegressor` easily does this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[63.81108808]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code task 5#\n",
    "#Fit the dummy regressor on the training data\n",
    "#Hint, call its `.fit()` method with `X_train` and `y_train` as arguments\n",
    "#Then print the object's `constant_` attribute and verify it's the same as the mean above\n",
    "dumb_reg = DummyRegressor(strategy='mean')\n",
    "dumb_reg.fit(X_train, y_train)\n",
    "dumb_reg.constant_\n",
    "#constant predicts one CONSTANT value so it only works w/ constant types like .mean\n",
    "\n",
    "\n",
    "#BUT - if you think about it, the x-points / features really don't matter because we're not even factoring those in /\n",
    "#taking anything away from them! but it doesn't give you the option to leave it out\n",
    "\n",
    "#but i think it's because for fitting purposes you have to have an x, and then these values are associated w/ /\n",
    "#affiliated w/ those, and so then whenever you call on the predictor to predict based off a set of / those set of\n",
    "#x-values, it'll be able to spit out the y that was affiliated w/ them\n",
    "#cuz when we're predicting, you're always gonna be calling on a set of X's!!! i.e. the REST of the data other than\n",
    "#what you're tryna predict - the INDEPENDENT VARIABLES!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wow. dead on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is this? How closely does this match, or explain, the actual values? There are many ways of assessing how good one set of values agrees with another, which brings us to the subject of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"How well does it explain the ACTUAL values?\" That is the question.\n",
    "\n",
    "#But also, this would only help us describe / learn the TRAINING data - NOT the TESTING DATA!!\n",
    "#may not tell us ANYTHING / may not be worth anything on the testing data!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.1 Metrics<a id='4.7.1_Metrics'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.7.1.1 R-squared, or coefficient of determination<a id='4.7.1.1_R-squared,_or_coefficient_of_determination'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One measure is $R^2$, the [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination). This is a measure of the proportion of variance in the dependent variable (our ticket price) that is predicted by our \"model\". The linked Wikipedia articles gives a nice explanation of how negative values can arise. This is frequently a cause of confusion for newcomers who, reasonably, ask how can a squared value be negative?\n",
    "\n",
    "Recall the mean can be denoted by $\\bar{y}$, where\n",
    "\n",
    "$$\\bar{y} = \\frac{1}{n}\\sum_{i=1}^ny_i$$\n",
    "\n",
    "and where $y_i$ are the individual values of the dependent variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aight so this is just the average price: the sum of the individual prices divided by the number of prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total sum of squares (error), can be expressed as\n",
    "\n",
    "$$SS_{tot} = \\sum_i(y_i-\\bar{y})^2$$\n",
    "\n",
    "The above formula should be familiar as it's simply the variance without the denominator to scale (divide) by the sample size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#right! cuz the variance is the 'average squared difference (with the mean/between the actual values and the mean)'\n",
    "#so okay we call this 'sum of squares'\n",
    "#so this is just the TOTAL or SUM OF the squared differences of/between the actual values and the average value!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residual sum of squares is similarly defined to be\n",
    "\n",
    "$$SS_{res} = \\sum_i(y_i-\\hat{y})^2$$\n",
    "\n",
    "where $\\hat{y}$ are our predicted values for the depended variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not sure why they wen't w/ 'residual' ... maybe because even w/ the best prediction there will inevitably still be\n",
    "#SOME difference w/ the actual values, some 'residual'\n",
    "#so this is the total sum of the squared differences between the ACTUAL AND PREDICTED >> so that 'variance',\n",
    "#although not literally / technically variance >> we can call this varia*TION*!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient of determination, $R^2$, here is given by\n",
    "\n",
    "$$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nice this is what i said!! except i didn't do the 1 - ...\n",
    "\n",
    "#so this is your predicted variance vs your actual\n",
    "\n",
    "#and is lower better (just looking at the ratio itself for a moment; not looking at the 1 -)? Well a perfect model\n",
    "#would exactly predict the actuals, so there WOULD BE no residual error/variance! thus SSres/the numerator would be 0!\n",
    "\n",
    "#and then theoretically there's no limit to how *bad* your prediction could be, so the sky is the limit for the numerator\n",
    "\n",
    "#so why the 1 - ?\n",
    "\n",
    "#that would just give us the balance/complement\n",
    "\n",
    "#so the ratio by itself is predicted variance over actual variance\n",
    "#so let's say the actual variance was 100, and the prediction brought it down to 10\n",
    "#so that ratio is 10/100, or .1\n",
    "#.1 is the portion of the variance predicted by our model\n",
    "#that's the definition for R^2 then isn't it? so why the 1 -?\n",
    "#1 - here would make our R2 = .9\n",
    "#so 90% of the variance is predicted by the model\n",
    "\n",
    "#well it looks like the 1 - minussing makes the r2 high when it's good\n",
    "#so let's think about that\n",
    "#so we know being able to have a predicted variance of just 1/10th of the actual is extremely good\n",
    "#so does the 90% mean it was simply 90% accurate in predicting the actuals? but isn't there a more direct way to do that?\n",
    "\n",
    "#so if the prediction results in only 10% variance compared to the original\n",
    "\n",
    "\n",
    "######################################################################################################################\n",
    "#AHHHHH okay i figured it out aH\n",
    "\n",
    "#so this ratio of predicted variation over actual, so that's telling us what portion of variance there STILL IS!!!\n",
    "#so it's stupid / misleading when it's said \"how much of the variance is \"DESCRIBED\" and that's why i got confused\n",
    "#this is what they mean - how much did the prediction improve, ameliorate, close the gap, RECONCILE the variance!!!\n",
    "\n",
    "#it's NOT literally \"WHAT PERCENT OF THE ACTUAL VARIANCE IS THE PREDICTED VARIANCE\" like they make it out to be / lead\n",
    "#you to believe. cuz then yeah, if your model was perfect and had ZER0 variance, then that ratio alone would be 0. and\n",
    "#yes 0 would mean/sound like it \"describes\" 0% of the variance. but the answer is 100%, because what they really mean is\n",
    "#this prediction is perfect and thus accounts for / closes ***100%*** of the variance/gap!!!\n",
    "\n",
    "\n",
    "#so likewise if the prediction is 30 and the actual was 100. then the prediction still has 30% variance remaining but\n",
    "#eliminated / reduced / fixed / scraped away 70% of it! and THAT'S what we wanna know!: how much / what percentage of\n",
    "#the original variance did the prediction ELIMINATE!\n",
    "\n",
    "#why couldn't they just use easy straightforward simple language and just say that???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so in this sense 'residual' variance makes sense. it's how much variance is left after the prediction / line / fit\n",
    "#and so that over the total is the PORTION/PERCENTAGE OF RESIDUAL VARIANCE / VARIANCE REMAINING!!!\n",
    "\n",
    "#and so 1 - that is the PORTION OF VARIANCE ELIMINATED BY THE PREDICTION! so that's the metric we wanna focus on\n",
    "#cuz it tells us directly how much work/good the prediction DID rather than focusing on what it DIDN'T DO!!!\n",
    "#think positive lol not negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it into words, it's one minus the ratio of the residual variance to the original variance. Thus, the baseline model here, which always predicts $\\bar{y}$, should give $R^2=0$. A model that perfectly predicts the observed values would have no residual error and so give $R^2=1$. Models that do worse than predicting the mean will have increased the sum of squares of residuals and so produce a negative $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r2 = 0 means the model didn't improve upon the actual AT ALL, that it literally has the EXACT SAME VARIANCE as\n",
    "#the training. and since the equation for variation for the actual / original data is the differences w/ the MEAN,\n",
    "#then a model that predicted the same exact thing would be ONE THAT USED THE MEAN AS THE PREDICTION FOR ALL THE / EVERY\n",
    "#SINGLE POINT!!! because the definition for the second one / the prediction variance is difference between the\n",
    "#predicted value and the actual, so if the predicted value IS THE MEAN, then it's the same definition as the original!!!!\n",
    "#difference between the MEAN and the actual!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code task 6#\n",
    "#Calculate the R^2 as defined above\n",
    "\n",
    "#okay so that's 1 - predicted variance / actual variance\n",
    "\n",
    "#it takes as the two arguments - (1) the training y's, and (2) the predicted y's\n",
    "def r_squared(y, ypred):\n",
    "    \"\"\"R-squared score.\n",
    "    \n",
    "    Calculate the R-squared, or coefficient of determination, of the input.\n",
    "    \n",
    "    Arguments:\n",
    "    y -- the observed (actual) values\n",
    "    ypred -- the predicted values\n",
    "    \"\"\"\n",
    "    #remember for context, when we say y we're talking about PRICES of tickets (\"AdultWeekend\")\n",
    "    #for ski resorts in America!\n",
    "    \n",
    "    #average y: sum of all actual y's divided by the number of y's\n",
    "    ybar = np.sum(y) / len(y) #yes, we could use np.mean(y)\n",
    "    #or could use y.mean()\n",
    "    \n",
    "    #total/sum of the squared differences b/w average\n",
    "    sum_sq_res = np.sum((y - ypred)**2) #residual sum of squares error\n",
    "    sum_sq_tot = np.sum((y - ybar)**2) #total sum of squares error\n",
    "    \n",
    "    #then we do 1 - the quotient of these 2!\n",
    "    R2 = 1.0 - sum_sq_res / sum_sq_tot\n",
    "    return R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make your predictions by creating an array of length the size of the training set with the single value of the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay so now we're gonna make our prediction of y_test and see what our r2 comes out to using the function we just made\n",
    "#so we're gonna predict y_test simply by saying all the y_test's are gonna be equal to the average of y_train!\n",
    "#and like we said above, this is gonna make for the exact same variation as y_train, thus our r2 will be 0, i.e./a.k.a.\n",
    "#our prediction didn't do jack to help improve the variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([63.81108808, 63.81108808, 63.81108808, 63.81108808, 63.81108808])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr_pred_ = train_mean * np.ones(len(y_train))\n",
    "y_tr_pred_[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is literally just copying and pasting / duplicating / dragging the singular mean down into a whole column!\n",
    "#by multiplying it by a bunch of / columns of 1's, or rather, multiplying a column of 1's by that one value to convert\n",
    "#all ones to train_mean :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is literally just copying and pasting / duplicating / dragging the singular mean down into a whole column!\n",
    "#by multiplying it by a bunch of / columns of 1's, or rather, multiplying a column of 1's by that one value to convert\n",
    "#all ones to train_mean :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([63.81108808, 63.81108808, 63.81108808, 63.81108808, 63.81108808])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                                  \n",
    "#this will return us the first 5 elements of this array\n",
    "#oh yeah so technically above wasn't a 'column' it's a ROW because it's an ARRAY! yes, it is! (joke cuz 'array' means no)\n",
    "#this is like how you'd do a \".head()\" preview of an array list!\n",
    "\n",
    "y_tr_pred_[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no surprise - it's all the same number!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the `sklearn` dummy regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([63.81108808, 63.81108808, 63.81108808, 63.81108808, 63.81108808])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr_pred = dumb_reg.predict(X_train)\n",
    "y_tr_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oh yeah! this is the same number that was predicted above because we just predicted it to be the mean! set the strategy\n",
    "#as 'mean'\n",
    "#but also, we got this number already earlier when we did the 'constant_' attribute\n",
    "\n",
    "#at first i was confused like why are we using X_train / the x-values, similar to what i said above,\n",
    "#when for this case our y values are being predicted SOLELY ON THE Y-VALUES THEMSELVES!!! NOT THEIR RELATIONSHIP W/\n",
    "#the x's! but again, in reality that's exactly what real prediction is! a function of the independent variables!\n",
    "#so/thus we need to call on the x-set; we fit a model by inputting the x-set so it's attached to that. so when we wanna\n",
    "#call on predict, we're saying, hey, based off of this set of x, can you predict the y?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that `DummyRegressor` produces exactly the same results and saves you having to mess about broadcasting the mean (or whichever other statistic we used - check out the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html) to see what's available) to an array of the appropriate length. It also gives you an object with `fit()` and `predict()` methods as well so you can use them as conveniently as any other `sklearn` estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#and now let's confirm that the r2 is 0, aka the prediction variation is the same as the original\n",
    "\n",
    "#so we have our two arguments - the training y's & the predicted y's\n",
    "#for the training y's, it doesn't matter here what the x's were; we're only interested in the variation between the\n",
    "#actual y's and the their average, and then in the second variation / resid variation, between the actual y's and the\n",
    "#predicted. they y_tr_pred didn't have to factor in the x's in this case, like we coulda used the straight set / array\n",
    "#of y-train-mean, but we're keeping it official/professional/profficianal\n",
    "#cuz to use the dummy regressor we needa do in terms of X_train. this has it baked in for us\n",
    "r_squared(y_train, y_tr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirmed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly as expected, if you use the average value as your prediction, you get an $R^2$ of zero _on our training set_. What if you use this \"model\" to predict unseen values from the test set? Remember, of course, that your \"model\" is trained on the training set; you still use the training set mean as your prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make your predictions by creating an array of length the size of the test set with the single value of the (training) mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0031235200417913944"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#okay, so just as above, where y_TRAIN_pred equals the array of constant train_mean\n",
    "#the TEST prediction will also be an array / single row of just the average training y -> train_mean\n",
    "#only difference is that we're making the array size the size of the TEST data rather than the training\n",
    "\n",
    "#because what's gonna happen is now we're gonna repeat this same process and see how it works for the TEST data:\n",
    "#see the variance between the original test values and ***THE TRAINING'S AVERAGE, **NOT** THE TEST'S AVERAGE,***\n",
    "#otherwise that's just gonna literally repeat what we just did and get us the exact same 0 R2 result\n",
    "#so we're gonna compare the residual variance between the test values and the prediction which is the training average\n",
    "#and compare it to the original variance between the test values and the test average (THERE'S where the test average\n",
    "#prices come into play!)\n",
    "\n",
    "#variance b/w test prices and training price average\n",
    "#____________________________________________________\n",
    "#variance b/w test prices and test price average\n",
    "\n",
    "#SO LET'S SEE IF USING THE TRAINING PRICE AVERAGE HELPED THE VARIANCE / WAS USEFUL AS A PREDICTOR!\n",
    "#so isn't that ANY R2 under 1, and greater than 0 is an IMPROVEMENT at least, i.e. it REDUCED the variation\n",
    "#R2 = 1.0 is PERFECT cuz that means it ELIMINATED ALL / 100% OF THE VARIATION - THERE *IS* NO RESIDUAL VARIATION LEFT!\n",
    "#and R2 = 0 is more like the \"break-even\", i.e. the prediction made 0 improvement - still has just as much variation\n",
    "#as the original\n",
    "#and when R2 is NEGATIVE, that means it's WORSE than the original! DISimproved the variation!! means the new prediction\n",
    "#variance is more than the original!\n",
    "\n",
    "\n",
    "y_te_pred = train_mean * np.ones(len(y_test))\n",
    "\n",
    "#NOTICE that it's taking the TRAINING mean once again and NOT the TEST MEAN! that's because remem we're using the\n",
    "#mean of the training data as our prediction. that prediction goes for our training set as well as our test\n",
    "#and actually, the doing it on the training set was really just to get practice using it and show it how it works\n",
    "#obviously it doesn't benefit us at all / give us any insight in the way of improving the variation / fitting / predicting\n",
    "#because it's literally just replicating the same data\n",
    "\n",
    "#where it really comes into play is here - we're seeing how good using the simple MEAN of the training prices is to\n",
    "#predict the test prices, and our measure of success is how much it improves the variance\n",
    "\n",
    "\n",
    "#okay so our r-squared function is taking as its argument for the two lists: (1) the y_test data,\n",
    "#which we haven't seen in a while but we defined WAY back in the beginning as just the remaining list of y-values, i.e.\n",
    "#PRICES that we saved for/as our test set\n",
    "#and (2), our \"predicted\" values which, again, are the AVERAGE of the training\n",
    "r_squared(y_test, y_te_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ouch. so we actually very slightly *hurt* our variation, did worse than our original using simple training average\n",
    "#as the prediction, but basically broke even / it's the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, you can expect performance on a test set to be slightly worse than on the training set. As you are getting an $R^2$ of zero on the training set, there's nowhere to go but negative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hmmm interesting. let's understand that. so the training set let's us break even exactly because we're literally just\n",
    "#replicating the values essentially - there's no improvement\n",
    "#so there's NO WAY our simple mean training value prediction can be better on the TEST set that it's not even at least\n",
    "#matching the average of! so yes of course it's gonna be at least a little worse!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^2$ is a common metric, and interpretable in terms of the amount of variance explained, it's less appealing if you want an idea of how \"close\" your predictions are to the true values. Metrics that summarise the difference between predicted and actual values are _mean absolute error_ and _mean squared error_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#true, so r2 tells you about variance, but doesn't actually tell you about how close your predictions are to the\n",
    "#actual values. the best fit line is obtained by minimizing the total squared difference to the line (variation), or\n",
    "#the average squared difference (variance) - both would give you the same\n",
    "\n",
    "#r2 gives you the variation, which is just like variance. and variation/variance is used to determine best fit line\n",
    "#by minimizing those. so then doesn't that mean it should be the best predictor?? bc the best predictor is the one\n",
    "#that minimizes the variation. so isn't that the same definition? the best fit/predictor line is the same thing as\n",
    "#tryna predict, and in both you're minimizing the variation/variance - so aren't the best/closest predictions what you'd\n",
    "#get w/ the best fit line?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.7.1.2 Mean Absolute Error<a id='4.7.1.2_Mean_Absolute_Error'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very simply the average of the absolute errors:\n",
    "\n",
    "$$MAE = \\frac{1}{n}\\sum_i^n|y_i - \\hat{y}|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay so rather than looking at the SQUARED error - average or total - we're looking at just ABSOLUTE difference between\n",
    "#the predicted values and the actual values\n",
    "\n",
    "#Code task 7#\n",
    "#Calculate the MAE as defined above\n",
    "def mae(y, ypred):\n",
    "    \"\"\"Mean absolute error.\n",
    "    \n",
    "    Calculate the mean absolute error of the arguments\n",
    "\n",
    "    Arguments:\n",
    "    y -- the observed/actual values\n",
    "    ypred -- the predicted values\n",
    "    \"\"\"\n",
    "    abs_error = np.abs(y - ypred)   #so the ypred is what we're gonna feed it / tell it is the predicted y's\n",
    "    mae = np.mean(abs_error)        #which is our y_tr_pred & y_te_pred\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.923463717146785"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the average absolute difference between the predicted values and the actual training values/prices\n",
    "#and, again, the predicted values are simply the average of the training prices\n",
    "mae(y_train, y_tr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so this would be your average absolute difference between predicted prices (average training) and actual prices\n",
    "#if you tried predicting training prices based on average price of the training\n",
    "\n",
    "#r2 is the portion amount of reconciled variation of predicted(average)/actuals over actuals/average\n",
    "#mae is the average absolute difference between predicted prices (average) and actual prices\n",
    "#so why was r2 = 0 when we did on training? cuz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.136142081278486"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the average absolute difference between the predicted values and the actual test values/prices\n",
    "#and, again, the predicted values are simply the average of the training prices\n",
    "mae(y_test, y_te_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so does it make sense that the test variance would be more? I guess so, because the prediction based on its own\n",
    "#average is gonna have to be better / more accurate than using that SAME average of a different data set on a\n",
    "#different data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean absolute error is arguably the most intuitive of all the metrics, this essentially tells you that, on average, you might expect to be off by around \\\\$19 if you guessed ticket price based on an average of known values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.7.1.3 Mean Squared Error<a id='4.7.1.3_Mean_Squared_Error'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common metric (and an important one internally for optimizing machine learning models) is the mean squared error. This is simply the average of the square of the errors:\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_i^n(y_i - \\hat{y})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so isn't mean squared error just variance? population variance at least\n",
    "\n",
    "#so whereas MAE was average absolute difference, MSE is average SQUARED difference\n",
    "\n",
    "#and vs variation which was the TOTAL squared difference\n",
    "\n",
    "#is there any metric for the total ABSOLUTE difference?\n",
    "\n",
    "#cuz there's total and average squared difference\n",
    "\n",
    "\n",
    "#oh wait, so maybe the differences are this:\n",
    "\n",
    "#variance/variation/standard deviation have to do with the DIFFERENCES BETWEEN THE POPULATION AND THEIR OWN MEAN!!!\n",
    "\n",
    "#whereas mae, mse, rms are the differences between the predicted values and the original values!\n",
    "\n",
    "#so it's a matter of comparing all values with ONE value\n",
    "#vs\n",
    "#comparing all values to ANOTHER SET of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Code task 8#\n",
    "#Calculate the MSE as defined above\n",
    "def mse(y, ypred):\n",
    "    \"\"\"Mean square error.\n",
    "    \n",
    "    Calculate the mean square error of the arguments\n",
    "\n",
    "    Arguments:\n",
    "    y -- the observed values\n",
    "    ypred -- the predicted values\n",
    "    \"\"\"\n",
    "    #the squared difference b/w the predicted y's and actual y's\n",
    "    #this takes two lists - one of each\n",
    "    #and so does this calculation for every pair in the list\n",
    "    #and then makes a new resultant list: sq_error\n",
    "    #and then we reduce / summarize that whole list down to one summary statistic/metric - the mean of that\n",
    "    #aka the *'mean squared error'*! aka mse!\n",
    "    sq_error = (ypred - y)**2\n",
    "    mse = np.mean(sq_error)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "614.1334096969057"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the mean squared error of the actual y's in the training set vs the predicted y's for the training set (average of\n",
    "#training y's)\n",
    "mse(y_train, y_tr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321.2505516198772"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#is that what we'd expect? i guess this should just be the square of mAe right?:\n",
    "mae(y_train, y_tr_pred) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.516376384950142e-15"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hmm okay i guess not\n",
    "\n",
    "#so let's think again, what is this - mse\n",
    "#mean squared error is the average squared difference between the predictions and the actuals\n",
    "#mean absolute error is the average absolute difference between the predictions and the actuals\n",
    "#well i guess it just doesn't work out that way\n",
    "#if you're taking the average of absolute differences and average of squared differences,\n",
    "#then you can't expect that the average of the SQUARED differences will be the square of the average absolute differences!\n",
    "#is there any relationship though? maybe for another time\n",
    "#i guess not though bec the issue comes that you're SUMMING them all together and that throws everything off\n",
    "\n",
    "#okay so if we're making the prediction as the average and comparing the actuals to it, isn't it like we're comparing\n",
    "#the actuals to themselves? cuz like when we have the average of a set, we can just treat the whole set / every member\n",
    "#of the set like it's the average. so if we're taking the average difference between the average and the actuals, then\n",
    "#wouldn't that average difference just be 0?\n",
    "\n",
    "#oh maybe if we did NON-absolute!\n",
    "\n",
    "\n",
    "def mnae(y, ypred):\n",
    "    \"\"\"Mean absolute error.\n",
    "    \n",
    "    Calculate the mean absolute error of the arguments\n",
    "\n",
    "    Arguments:\n",
    "    y -- the observed/actual values\n",
    "    ypred -- the predicted values\n",
    "    \"\"\"\n",
    "    nonabs_error = y - ypred   #so the ypred is what we're gonna feed it / tell it is the predicted y's\n",
    "    mnae = np.mean(nonabs_error)        #which is our y_tr_pred & y_te_pred\n",
    "    return mnae\n",
    "\n",
    "mnae(y_train, y_tr_pred)\n",
    "\n",
    "#aH yepp! you got it! it would just be 0!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and how about the average squared difference?\n",
    "#oh wait, that's taking absolute difference again, so no, wouldn't be 0, cuz will always be positive! never negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "581.4365441953481"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mean squared error of our TEST set\n",
    "mse(y_test, y_te_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hunh, that's weird. so the the mse was actually LESS on the test set than on the training? whereas the mAe was\n",
    "#more for the test\n",
    "#why would the squared error be less for the test set, but the absolute error was more?\n",
    "#because the training is comparing the average of the training prices to the individual training prices themselves,\n",
    "#whereas the test is comparing the average of the TRAINING prices to the individual test prices,\n",
    "#so how could the test prices be closer to the training prices THAN THE TRAINING PRICES???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here, you get a slightly better MSE on the test set than you did on the train set. And what does a squared error mean anyway? To convert this back to our measurement space, we often take the square root, to form the _root mean square error_ thus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROOT MEAN SQUARE! i always hear about this. so it's simply the SQUARE ROOT of the average squared difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.78171523, 24.11299534])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#writing it this way simply gives us two things/outputs at once:\n",
    "#the rms of the training set vs. the rms of the test set\n",
    "np.sqrt([mse(y_train, y_tr_pred), mse(y_test, y_te_pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so when you look at it like this, it's basically identical! closes the gaps, warps it down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.2 sklearn metrics<a id='4.7.2_sklearn_metrics'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions are good, but you don't want to have to define functions every time we want to assess performance. `sklearn.metrics` provides many commonly used metrics, included the ones above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nice, so rather than define our own functions, use the built-in ones from slickitlearn!\n",
    "#but it's good that we did it manually to get that practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.7.2.0.1 R-squared<a id='4.7.2.0.1_R-squared'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, -0.0031235200417913944)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.7.2.0.2 Mean absolute error<a id='4.7.2.0.2_Mean_absolute_error'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.92346371714677, 19.136142081278486)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.7.2.0.3 Mean squared error<a id='4.7.2.0.3_Mean_squared_error'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(614.1334096969046, 581.4365441953483)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok wait so all these above metrics are simply called like that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.3 Note On Calculating Metrics<a id='4.7.3_Note_On_Calculating_Metrics'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calling functions to calculate metrics, it is important to take care in the order of the arguments. Two of the metrics above actually don't care if the arguments are reversed; one does. Which one cares?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maybe r2 because r2 uses predicted variation / original variation, so it takes 2 args - predicted and actuals\n",
    "#thus IT NEEDS TO KNOW WHICH ONE IS THE PREDICTED AND WHICH IS THE ACTUAL!\n",
    "#because it's gonna do variance of predicted vs actuals over variance of actuals vs their average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so let's switch it up and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.041041349306602e+30, 0.0)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_tr_pred, y_train), r2_score(y_te_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.92346371714677"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_tr_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "614.1334096969046"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_tr_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oh wow! ok so the second 2 are EXACTLY the same, but the first one, r2, is COMPLETELY different! it's a GIGANTIC\n",
    "#NUMBER!!! so were right that order matters here\n",
    "#the reason why it's gigantic is because the denominator becomes switched to be diff b/w predicted values, which\n",
    "#is just the training values' average, and the predicted values' average, which of course is the same number\n",
    "#so of course the denominator is gonna be 0 - so shouldn't that raise a divide by 0 error??\n",
    "\n",
    "#maybe instead of raising a divide by 0 error, it just treats it as a really small number, making the quotient\n",
    "#a really huge number, making 1 minus that number a hugely negative number?\n",
    "#but then the same thing should happen for the other scenario - incorrect order for r2 sicklitearn function on test -\n",
    "#but that just evaluates to 0\n",
    "\n",
    "#but as we see below, a div/0 error IS raised but for some reason ONLY in one of the four incorrect configuration\n",
    "#situations - when we use our homebrew function for test set.... thought they should all 4 be div/0 errors\n",
    "\n",
    "#cuz/and then r2 for test should be the same situation that ends in divide by 0 because its denominator is also gonna be,\n",
    "#when slated in the incorrect order, the variation between the predicted values, which are simply the training avg,\n",
    "#thus they're all the same one number, and the average of those numbers, which of course is the same number, thus\n",
    "#the variation would be 0 there too!\n",
    "\n",
    "#i guess it's like how doing the percent change between 2 numbers is different depending on which one's the starting\n",
    "#and which is the ending. same thing here - you're seeing how much things changed relative to where they started\n",
    "\n",
    "\n",
    "#ahh okay, i just wrote it out and i see why it'd be different mathematically\n",
    "\n",
    "#so r2 has pred var over / orig var\n",
    "\n",
    "#the predicted var would be the same cuz it's just the SQUARED diff, thus it wouldn't matter if you mixed up orig\n",
    "#and pred, cuz the squared diff will be the same, will always be positive, order doesn't matter\n",
    "\n",
    "#but the DENOMINATOR is where the difference arises. it's supposed to be the variation of the originals with their own\n",
    "#average, so if you mix it up w/ predicted, it's gonna be the variation w/ THE PREDICTED'S AVERAGE! so of course\n",
    "#that's gonna give you a diff num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Jupyter code cell, running `r2_score?` will bring up the docstring for the function, and `r2_score??` will bring up the actual code of the function! Try them and compare the source for `sklearn`'s function with yours. Feel free to explore what happens when you reverse the order of the arguments and compare behaviour of `sklearn`'s function and yours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oh wow that's cool, i never knew jupyter had its own documentation! let's try:\n",
    "r2_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so what's the difference between the single & double question marks?\n",
    "#they seem to give you essentially the same exact information/layout\n",
    "#ok but it does look like the ?? double gives you more details/info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, -3.041041349306602e+30)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train set - sklearn\n",
    "# correct order, incorrect order\n",
    "r2_score(y_train, y_tr_pred), r2_score(y_tr_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay homebrew & sklearn both give 0 when in correct order and both give this ginormous negative number when in wrong order\n",
    "#we'll try to see why below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0031235200417913944, 0.0)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test set - sklearn\n",
    "# correct order, incorrect order\n",
    "r2_score(y_test, y_te_pred), r2_score(y_te_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn & homebrew both give .003 when in correct order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, -3.041041349306602e+30)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train set - using our homebrew function\n",
    "# correct order, incorrect order\n",
    "r_squared(y_train, y_tr_pred), r_squared(y_tr_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.041041349306602e+30"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so what happens when we switch order?\n",
    "\n",
    "#remem, r_squared is:\n",
    "\n",
    "# def r_squared(y, ypred):\n",
    "#     \"\"\"R-squared score.\n",
    "    \n",
    "#     Calculate the R-squared, or coefficient of determination, of the input.\n",
    "    \n",
    "#     Arguments:\n",
    "#     y -- the observed values\n",
    "#     ypred -- the predicted values\n",
    "#     \"\"\"\n",
    "#     ybar = np.sum(y) / len(y) #yes, we could use np.mean(y)\n",
    "#     sum_sq_tot = np.sum((y - ybar)**2) #total sum of squares error\n",
    "#     sum_sq_res = np.sum((y - ypred)**2) #residual sum of squares error\n",
    "\n",
    "#so when we do r_squared(y_tr_pred, y_train),\n",
    "\n",
    "#y, aka y actuals = y_tr_pred, i.e. it thinks the actuals are the average training value\n",
    "#ypred = y_train, i.e. the ACTUAL original training values\n",
    "\n",
    "#so it becomes:\n",
    "\n",
    "yavg = np.sum(y_tr_pred) / len(y_tr_pred) #thus y_avg that will be used in denom is the average of the already\n",
    "#averaged set of one number, so it's just gonna be that same number that y_tr_pred is already composed of!\n",
    "\n",
    "\n",
    "#numerator\n",
    "sum_sq_resid = np.sum((y_tr_pred - y_train)**2) #residual sum of squares error\n",
    "#the sum of the squared differences between the average of the training values and the training values\n",
    "\n",
    "\n",
    "#denominator\n",
    "sum_sq_totes = np.sum((y_tr_pred - yavg)**2) #the sum of the squared diff b/w the average value and the average value,\n",
    "#i.e. 0!!!\n",
    "\n",
    "#r2\n",
    "r_sq = 1 - sum_sq_resid / sum_sq_totes\n",
    "\n",
    "r_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whaaat okay well at least this means i calculated it right. but let's break it down further into components to see\n",
    "#what's doing it exactly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([63.81108808, 63.81108808, 63.81108808, 63.81108808, 63.81108808])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.81108808290154"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yavg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118527.74807150259"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_sq_resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.897603960515975e-26"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_sq_totes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hmm, okay, so the denom is what we'd expect, which was the harder part, and it's essentially 0\n",
    "#not sure why it isn't exactly\n",
    "#but that would explain why it's some a-hayuge number and also thus why we didn't get a divide/0 num\n",
    "\n",
    "#and the numerator is the same as it would be as if we did it the correct order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so we don't even have to test the 'test' set (lol),\n",
    "#we know that the reason it gives the divide/0 error using our formula is because the denom actually DOES compute to\n",
    "#0. why it's different? not sure:\n",
    "\n",
    "\n",
    "yavg_test = np.sum(y_te_pred) / len(y_te_pred)\n",
    "sum_sq_totes_test = np.sum((y_te_pred - yavg_test)**2)\n",
    "sum_sq_totes_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.811088082901556"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yavg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ahh okay, here's our answer on why they don't evaluate the same. like yes, i'm right aH, they all SHOULD evaluate to\n",
    "#denom = 0 and throw a div/0 error, but for some reason, chotay motay decimals can be slightly different that changes\n",
    "#everything - makes the difference between something being ACTUALLY 0 and throwing a div/0 error vs saying the ANSWER\n",
    "#is 0 vs a insanely LARGE number as a result of the denominator being so small\n",
    "\n",
    "#so apparently the slickitlearn function, when it sees that the denominator is 0, it just turns the whole thing 0?\n",
    "#cuz its r2 function evaluates to 0, which would mean the quotient/ratio is **1**, cuz r2 = 1 - quotient\n",
    "\n",
    "#so idk how it converts a super small number to simply be the same as its numerator? but its as if it knows this should\n",
    "#really be 0 and thus a divideby/0 error and so it just overrides everything and calls the r2 0. cuz 0 means the\n",
    "#prediction didn't help at all, which i guess isn't exactly true if you mixed it up, it should be negative, but this\n",
    "#just ignores/avoids all that by just simply negating all of it and making it 0\n",
    "#because, if mixed up, the 'pred' var over the 'orig' var would be that it got worse because the ORIGINAL WAS PERFECT!!!\n",
    "#HAD 0 VARIATION!!! THUS IT WAS ALL DOWNHILL FROM THERE!!!\n",
    "#and actually that's why it actually makes sense that r2 would be a gigantic negative number because negative means you\n",
    "#made the variation worse so this would be true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yeah so if you look in the docstring for r2? it does say i believe that its built in to default to 0 to avoid\n",
    "#a divide by 0 error / infinite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([63.81108808, 63.81108808, 63.81108808, 63.81108808, 63.81108808])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#same preds used for training\n",
    "y_te_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cr/9gwdd0wn43d9d1db4xfhyvpc0000gn/T/ipykernel_4208/2615140823.py:28: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  R2 = 1.0 - sum_sq_res / sum_sq_tot\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.0031235200417913944, -inf)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test set - using our homebrew function\n",
    "# correct order, incorrect order\n",
    "r_squared(y_test, y_te_pred), r_squared(y_te_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yep because if we do the wrong order, denom in r2 equation will be 0!!! cuz getting variance of a set of values\n",
    "#that's all the same one value and comparing it TO THAT VALUE!!! thus 0 variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get very different results swapping the argument order. It's worth highlighting this because data scientists do this too much in the real world! Don't be one of them! Frequently the argument order doesn't matter, but it will bite you when you do it with a function that does care. It's sloppy, bad practice and if you don't make a habit of putting arguments in the right order, you will forget!\n",
    "\n",
    "Remember:\n",
    "* argument order matters,\n",
    "* check function syntax with `func?` in a code cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meaning the name of the function, any function, followed by a question mark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 Initial Models<a id='4.8_Initial_Models'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8.1 Imputing missing feature (predictor) values<a id='4.8.1_Imputing_missing_feature_(predictor)_values'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall when performing EDA, you imputed (filled in) some missing values in pandas. You did this judiciously for exploratory/visualization purposes. You left many missing values in the data. You can impute missing values using scikit-learn, but note that you should learn values to impute from a train split and apply that to the test split to then assess how well your imputation worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ahh okay, so when we have missing values, don't fill them all in at once\n",
    "#first split the data into training/test, and learn the training data to base the fill value on\n",
    "#and then see how good those fill values were based on performance of the test set\n",
    "\n",
    "#cuz remem, the basic idea is that:\n",
    "\n",
    "#we have a dataset\n",
    "#we use most of it to learn/train\n",
    "#we come up w/ a fit/model\n",
    "#then we see how well that model fits/predicts the remaining/un-before-seen test set\n",
    "\n",
    "\n",
    "#BUTTT - when you look below, we're just applying the same set of medians to both train & test - not the train w/ train\n",
    "#and test w/ test..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.8.1.1 Impute missing values with median<a id='4.8.1.1_Impute_missing_values_with_median'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so we have diff ways/methods we can fill in unknown / missing values\n",
    "#can treat them as 0\n",
    "#can make them the average of the rest - kinda like how we used that as a basic predictor method\n",
    "#can use the median of the rest\n",
    "#etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's missing values. Recall from your data exploration that many distributions were skewed. Your first thought might be to impute missing values using the median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.8.1.1.1 Learn the values to impute from the train set<a id='4.8.1.1.1_Learn_the_values_to_impute_from_the_train_set'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "summit_elev                           2215.000000\n",
       "vertical_drop                          750.000000\n",
       "base_elev                             1300.000000\n",
       "trams                                    0.000000\n",
       "fastSixes                                0.000000\n",
       "fastQuads                                0.000000\n",
       "quad                                     1.000000\n",
       "triple                                   1.000000\n",
       "double                                   1.000000\n",
       "surface                                  2.000000\n",
       "total_chairs                             7.000000\n",
       "Runs                                    28.000000\n",
       "TerrainParks                             2.000000\n",
       "LongestRun_mi                            1.000000\n",
       "SkiableTerrain_ac                      170.000000\n",
       "Snow Making_ac                          96.500000\n",
       "daysOpenLastYear                       109.000000\n",
       "yearsOpen                               57.000000\n",
       "averageSnowfall                        120.000000\n",
       "projectedDaysOpen                      115.000000\n",
       "NightSkiing_ac                          70.000000\n",
       "Total_Resorts                           15.000000\n",
       "resorts_per_100kcapita                   0.248243\n",
       "resorts_per_100ksq_mile                 22.902162\n",
       "resort_skiable_area_ac_state_ratio       0.051458\n",
       "resort_days_open_state_ratio             0.071225\n",
       "resort_terrain_park_state_ratio          0.069444\n",
       "resort_night_skiing_state_ratio          0.077081\n",
       "total_chairs_runs_ratio                  0.200000\n",
       "total_chairs_skiable_ratio               0.040323\n",
       "fastQuads_runs_ratio                     0.000000\n",
       "fastQuads_skiable_ratio                  0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the values we'll use to fill in any missing values\n",
    "X_defaults_median = X_train.median()   #remember 'X_train' way back from the beginning? all the features / values\n",
    "X_defaults_median                      #columns to be used to predict price BESIDES of course price, and the training\n",
    "                                       #split portion of that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALHAMDULILLAH! EVERYTHING MATCHES!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMEMBER BY THE WAY - we DID delete out and rows that had missing PRICES!!! this is just talking about how to deal with\n",
    "#the missings in ALL THE OTHER COLUMNS!!!\n",
    "\n",
    "#and remem, the training and test sets split from the same, parent, unified set. so they BOTH have the same\n",
    "#columns/rows, i.e. no missing prices, and the x_train & x_test are both every feature but price,\n",
    "#and y_train & y_test are both price\n",
    "\n",
    "#so we're gonna try to see eventually how well we can fit a model to predict price based on the training data and see\n",
    "#how well that fares on the testing data! a fresh set / practice test not seen before to see how well it can really think\n",
    "#for itself / understood the information!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.8.1.1.2 Apply the imputation to both train and test splits<a id='4.8.1.1.2_Apply_the_imputation_to_both_train_and_test_splits'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oh ok so i was gonna say like but aren't we supposed to first learn the training set and THEN apply it on the test\n",
    "#but since we're doing the MEDIAN to backfill, we're not using any kind of real learning from the training set!\n",
    "#we're using the OVERALL MEDIANS and applying it to fill ALL THE NANS from each column! only reason we have to write it\n",
    "#out separately here is bc way back in the beginning w/ the train-test-learn function that split things up right there\n",
    "#and then but we're doing the exact same thing to them!\n",
    "\n",
    "#similar to before how we used the straight average of the training set for prediction of the test set,\n",
    "#similarly here we're gonna use the median of each column to fill in the nans of both training & test\n",
    "#(but remem, it's NOT that we're using training set to backfill the training set and likewise test's to backfill test;\n",
    "#we're filling ALL/BOTH w/ the SAME SET OF MEDIANS!!! treating them as their original, one, unified set!!!\n",
    "#so don't get confused - this isn't like PREDICTING earlier where we used the TRAINING set's average as a predictor\n",
    "#for the test. that makes sense there\n",
    "\n",
    "#but for BACKFILLING MISSINGS, we wouldn't need to do that - just use the median of the dataset / portion in question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code task 9#\n",
    "\n",
    "#so remember X_train & X_test are what we defined earlier as the predicting features/values sets in the training & test\n",
    "#split respectively\n",
    "\n",
    "#so those are apart of the train_test_split skilclitearn function\n",
    "\n",
    "#so those have methods of their own, which we're using below, to fill in any missing values!\n",
    "\n",
    "#Call `X_train` and `X_test`'s `fillna()` method, passing `X_defaults_median` as the values to use\n",
    "#Assign the results to `X_tr` and `X_te`, respectively\n",
    "X_tr = X_train.fillna(X_defaults_median)\n",
    "X_te = X_test.fillna(X_defaults_median)\n",
    "\n",
    "#oh shoot so i think i messed up in my explanations in other places. so this IS like before -\n",
    "#we're not using the whole / parent medians for each column to back fill both the training and test data\n",
    "#like before, we're only taking the TRAINING medians, and fitting on the TRAINING set, and then using the SAME\n",
    "#training medians to fill in the test blanks!!!\n",
    "\n",
    "\n",
    "#okay so we're simply telling it fill the na's in each column of X_train/test (all the non-price features)\n",
    "#w/ that respective column's MEDIAN, which is/as defined by / laid out in 'X_defaults_median'\n",
    "\n",
    "#and note here that BOTH the training and test sets will get the SAME MEDIAN VALUES FILLED IN THEIR NA'S FOR THE SAME\n",
    "#COLUMNS!\n",
    "\n",
    "#so, i was gonna say what was the point of splitting these off now then, but we split tese up in the VERY BEGINNING!\n",
    "#thus we already had 2 sets to work with so have to deal w/ each of them separately\n",
    "\n",
    "\n",
    "#so it's not like we're first doing something on the training set, using its median, then doing the same w/ the test\n",
    "#set, i.e. using the TEST SET'S median! they're both part of the SAME parent unified set and so are BOTH getting the SAME\n",
    "#MEDIAN vals!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.8.1.1.3 Scale the data<a id='4.8.1.1.3_Scale_the_data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have features measured in many different units, with numbers that vary by orders of magnitude, start off by scaling them to put them all on a consistent scale. The [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) scales each feature to zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so right, this is to SRHINK down the various different columns to use the same scale so that they're all still the\n",
    "#same values relative to each other within each/same feature/column, but now we can also compare them to EACH OTHER\n",
    "#for easy correlation/trending/plotting!!!\n",
    "\n",
    "#Code task 10#\n",
    "#Call the StandardScaler`s fit method on `X_tr` to fit the scaler\n",
    "#then use its `transform()` method to apply the scaling to both the train and test split\n",
    "#data (`X_tr` and `X_te`), naming the results `X_tr_scaled` and `X_te_scaled`, respectively\n",
    "\n",
    "\n",
    "#so StandardScaler() is a built-in function that will standardize and scale our dataset\n",
    "\n",
    "#make it into a more concise variable\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#call SS's 'fit' method to both do the actual scaling AND come up w/ the best fit line(s)/PCs?\n",
    "#combined 2 steps in 1?\n",
    "scaler.fit(X_tr)\n",
    "\n",
    "#^so somehow this will magically look at all 200+ resorts and 30+ features and come up w/ the best fit lines\n",
    "#for each dimension, i.e. a Principle Component\n",
    "#since there's much fewer features than there are resorts, it's gonna look at each feature as a dimension,\n",
    "#to get a PC on, as we saw earlier in Unit 3\n",
    "\n",
    "\n",
    "#and so it'll get those set of best fit lines for the scaled features based on the number of those features\n",
    "#and remem in pca it gets the variance on each of those PCs to tell us where the most significant ones are!\n",
    "\n",
    "#and remem, it's not about the variance from EACH OTHER across/within ONE feature, it's about the COMBINED / VECTORED (eigenvector?? linear combination??)\n",
    "#variance from each other in the plot of TWO FEATURES - so the 'each other' here is an ORDERED PAIR of (feat1, feat2)!\n",
    "#and of course the TWO DIFFERENT AXISES become ONE >> the eigenvector/linear combination? and that new one single axis,\n",
    "#off which the variance is based, is the PRINCIPLE COMPONENT!\n",
    "#and so for each feature there's a new principle component\n",
    "#and they all get ranked in terms of how much variance they're responsible\n",
    "#the more variance there is in the component features, the more variance we can expect from the PC!\n",
    "#^i believe that's true, at least that's a nice simplistic understanding - cuz remember PCA is showing us / grouping by\n",
    "#what matters most and what matters most are the most POLARIZING fields, i.e. the ones w/ the most variance\n",
    "#but remem, we don't have a way to fairly compare the variance of the indiv features bc of the diff scales and\n",
    "#stdv scaling makes the variance 0! \n",
    "\n",
    "\n",
    "#what was the point of writing up there by itself if we were just gonna do it down here and assign to variables?\n",
    "#that was because i had it wrong down here and it's supposed to be TRANSFORM down here - fixed now!\n",
    "\n",
    "\n",
    "\n",
    "#okay and now that that SCALED & FITTED the data,\n",
    "#this is gonna TRANSFORM THE data, i.e. twist it to line up w/ its new axes >> the PCs!!\n",
    "#so each point will have a new set of ***32***-length ordered-'pair' / coordinate address\n",
    "#one representing each axis/PC, which represents the number of features!!!\n",
    "\n",
    "\n",
    "\n",
    "X_tr_scaled = scaler.transform(X_tr)   #originally put .fit() for these both for some reason -__-\n",
    "X_te_scaled = scaler.transform(X_te)\n",
    "\n",
    "\n",
    "#.transform() REMAPS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay but wait - so X_tr_scaled is scaler.fit(X_tr),\n",
    "#so then is X_tr_scaled equaled to the SCALED set of X_train values or the\n",
    "#PC FIT LINES???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.91993102, -0.90945616, -0.83060204, ...,  0.36484513,\n",
       "        -0.46094631, -0.38882899],\n",
       "       [-0.88912245, -0.884982  , -0.80423909, ...,  2.96389608,\n",
       "        -0.46094631, -0.38882899],\n",
       "       [ 0.40091624, -0.26089079,  0.54521454, ...,  0.1717268 ,\n",
       "        -0.46094631, -0.38882899],\n",
       "       ...,\n",
       "       [-0.57935632, -0.05286039, -0.66385637, ...,  0.01539291,\n",
       "         4.51469512,  6.05647867],\n",
       "       [-0.82274399, -0.95228595, -0.70801431, ...,  0.56869227,\n",
       "        -0.46094631, -0.38882899],\n",
       "       [-0.90312634, -0.84827075, -0.83060204, ...,  0.67061583,\n",
       "        -0.46094631, -0.38882899]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(193, 32)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_tr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whatHHH? it's the same thing\n",
    "#so:\n",
    "# scaler = StandardScaler() = X_tr_scaled = scaler.transform(X_tr) ###originally had last method as .fit() instead of .transform()!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#so okay, i guess last hope is to try scaler.fit(X_tr):\n",
    "#scaler.transform(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph, still the same thing...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summit_elev</th>\n",
       "      <th>vertical_drop</th>\n",
       "      <th>base_elev</th>\n",
       "      <th>trams</th>\n",
       "      <th>fastSixes</th>\n",
       "      <th>fastQuads</th>\n",
       "      <th>quad</th>\n",
       "      <th>triple</th>\n",
       "      <th>double</th>\n",
       "      <th>surface</th>\n",
       "      <th>...</th>\n",
       "      <th>resorts_per_100kcapita</th>\n",
       "      <th>resorts_per_100ksq_mile</th>\n",
       "      <th>resort_skiable_area_ac_state_ratio</th>\n",
       "      <th>resort_days_open_state_ratio</th>\n",
       "      <th>resort_terrain_park_state_ratio</th>\n",
       "      <th>resort_night_skiing_state_ratio</th>\n",
       "      <th>total_chairs_runs_ratio</th>\n",
       "      <th>total_chairs_skiable_ratio</th>\n",
       "      <th>fastQuads_runs_ratio</th>\n",
       "      <th>fastQuads_skiable_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>790</td>\n",
       "      <td>300</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248243</td>\n",
       "      <td>16.103800</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.065101</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>900</td>\n",
       "      <td>320</td>\n",
       "      <td>580</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280368</td>\n",
       "      <td>28.951341</td>\n",
       "      <td>0.003631</td>\n",
       "      <td>0.019674</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.008222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>5506</td>\n",
       "      <td>830</td>\n",
       "      <td>4675</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057208</td>\n",
       "      <td>11.148479</td>\n",
       "      <td>0.256757</td>\n",
       "      <td>0.193676</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.283582</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.084211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>10488</td>\n",
       "      <td>2494</td>\n",
       "      <td>7994</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405495</td>\n",
       "      <td>15.312673</td>\n",
       "      <td>0.039334</td>\n",
       "      <td>0.104275</td>\n",
       "      <td>0.069444</td>\n",
       "      <td>0.077081</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.003333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2600</td>\n",
       "      <td>1540</td>\n",
       "      <td>1200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410091</td>\n",
       "      <td>0.450867</td>\n",
       "      <td>0.280702</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.077081</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>11952</td>\n",
       "      <td>1162</td>\n",
       "      <td>10790</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382028</td>\n",
       "      <td>21.134744</td>\n",
       "      <td>0.018314</td>\n",
       "      <td>0.043892</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.077081</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.008750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>2000</td>\n",
       "      <td>700</td>\n",
       "      <td>1300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169635</td>\n",
       "      <td>60.489414</td>\n",
       "      <td>0.015415</td>\n",
       "      <td>0.071225</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.025740</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2006</td>\n",
       "      <td>1000</td>\n",
       "      <td>1006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159594</td>\n",
       "      <td>104.225886</td>\n",
       "      <td>0.096055</td>\n",
       "      <td>0.071225</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.178388</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.026786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>1137</td>\n",
       "      <td>265</td>\n",
       "      <td>872</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.257624</td>\n",
       "      <td>22.902162</td>\n",
       "      <td>0.034286</td>\n",
       "      <td>0.044766</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.056338</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>850</td>\n",
       "      <td>350</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1.176721</td>\n",
       "      <td>171.141299</td>\n",
       "      <td>0.014006</td>\n",
       "      <td>0.056849</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.061170</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     summit_elev  vertical_drop  base_elev  trams  fastSixes  fastQuads  quad  \\\n",
       "133          790            300        500      0          0          0     1   \n",
       "120          900            320        580      0          0          0     0   \n",
       "218         5506            830       4675      0          0          0     3   \n",
       "269        10488           2494       7994      0          0          4     2   \n",
       "1           2600           1540       1200      0          0          0     0   \n",
       "..           ...            ...        ...    ...        ...        ...   ...   \n",
       "37         11952           1162      10790      0          0          0     1   \n",
       "209         2000            700       1300      0          0          0     0   \n",
       "94          2006           1000       1006      0          0          3     0   \n",
       "314         1137            265        872      0          0          0     0   \n",
       "163          850            350        500      0          0          0     0   \n",
       "\n",
       "     triple  double  surface  ...  resorts_per_100kcapita  \\\n",
       "133       0       2        3  ...                0.248243   \n",
       "120       2       1        2  ...                0.280368   \n",
       "218       0       3        2  ...                0.057208   \n",
       "269       1       1        1  ...                0.405495   \n",
       "1         0       4        0  ...                0.410091   \n",
       "..      ...     ...      ...  ...                     ...   \n",
       "37        0       4        2  ...                0.382028   \n",
       "209       1       1        3  ...                0.169635   \n",
       "94        1       0        4  ...                0.159594   \n",
       "314       1       1        5  ...                0.257624   \n",
       "163       3       0        3  ...                1.176721   \n",
       "\n",
       "     resorts_per_100ksq_mile  resort_skiable_area_ac_state_ratio  \\\n",
       "133                16.103800                            0.038462   \n",
       "120                28.951341                            0.003631   \n",
       "218                11.148479                            0.256757   \n",
       "269                15.312673                            0.039334   \n",
       "1                   0.450867                            0.280702   \n",
       "..                       ...                                 ...   \n",
       "37                 21.134744                            0.018314   \n",
       "209                60.489414                            0.015415   \n",
       "94                104.225886                            0.096055   \n",
       "314                22.902162                            0.034286   \n",
       "163               171.141299                            0.014006   \n",
       "\n",
       "     resort_days_open_state_ratio  resort_terrain_park_state_ratio  \\\n",
       "133                      0.065101                         0.137931   \n",
       "120                      0.019674                         0.015873   \n",
       "218                      0.193676                         0.111111   \n",
       "269                      0.104275                         0.069444   \n",
       "1                        0.130435                         0.250000   \n",
       "..                            ...                              ...   \n",
       "37                       0.043892                         0.027027   \n",
       "209                      0.071225                         0.027778   \n",
       "94                       0.071225                         0.111111   \n",
       "314                      0.044766                         0.100000   \n",
       "163                      0.056849                         0.046512   \n",
       "\n",
       "     resort_night_skiing_state_ratio  total_chairs_runs_ratio  \\\n",
       "133                         0.058824                 0.400000   \n",
       "120                         0.008222                 0.333333   \n",
       "218                         0.283582                 0.470588   \n",
       "269                         0.077081                 0.112500   \n",
       "1                           0.077081                 0.111111   \n",
       "..                               ...                      ...   \n",
       "37                          0.077081                 0.109375   \n",
       "209                         0.025740                 0.227273   \n",
       "94                          0.178388                 0.296296   \n",
       "314                         0.056338                 0.388889   \n",
       "163                         0.061170                 0.352941   \n",
       "\n",
       "     total_chairs_skiable_ratio  fastQuads_runs_ratio  fastQuads_skiable_ratio  \n",
       "133                    0.100000              0.000000                 0.000000  \n",
       "120                    0.312500              0.000000                 0.000000  \n",
       "218                    0.084211              0.000000                 0.000000  \n",
       "269                    0.007500              0.050000                 0.003333  \n",
       "1                      0.006250              0.000000                 0.000000  \n",
       "..                          ...                   ...                      ...  \n",
       "37                     0.008750              0.000000                 0.000000  \n",
       "209                    0.058824              0.000000                 0.000000  \n",
       "94                     0.071429              0.111111                 0.026786  \n",
       "314                    0.116667              0.000000                 0.000000  \n",
       "163                    0.125000              0.000000                 0.000000  \n",
       "\n",
       "[193 rows x 32 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#well let's atleast see X_tr\n",
    "X_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay, well, aside from the weird unexplained order, the values look normal and this should have as we programmed,\n",
    "#the blanks to be filled w/ their column's median (of the non-blank values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so how would i see JUST the scaled values, BEFORE they get transformed?\n",
    "#cuz when we did this- scaler.fit(X_tr) -it just showed us that it was a StandardScaler() function\n",
    "#maybe there's something else you have to call on it to reveal the scaled numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.8.1.1.4 Train the model on the train split<a id='4.8.1.1.4_Train_the_model_on_the_train_split'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay, so now that we have our SCALED AND TRANSFORMED training data,\n",
    "#we're gonna bring in our long awaited, main event - PRICES! the thing we're tryna predict. our y, dependent variable\n",
    "#so we're gonna train/learn from the X's and their relationship w/ their respective y's/prices to make our model / our\n",
    "#linear regressing best fit line\n",
    "#how is that gonna take the new coords and add a y to already 32 dimensions?? or even if you just had 2 PCs??\n",
    "#i'm not sure\n",
    "#maybe we're taking just the FIRST PC as our x?? \n",
    "\n",
    "#okay so i guess the \".fit()\" above was NOT to get PCs/PC lines/variance! that fit was to simply to CREATE the\n",
    "#/SCALE the data!\n",
    "\n",
    "#it's like the '.fit()' feature in alot of these functions is simply like the starting point, the initiation to\n",
    "#get things going\n",
    "\n",
    "lm = LinearRegression().fit(X_tr_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.8.1.1.5 Make predictions using the model on both train and test splits<a id='4.8.1.1.5_Make_predictions_using_the_model_on_both_train_and_test_splits'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code task 11#\n",
    "#Call the `predict()` method of the model (`lm`) on both the scaled train data and scaled test data\n",
    "#Assign the predictions to `y_tr_pred` and `y_te_pred`, respectively\n",
    "\n",
    "#okay so lm is the linear regression model based on X training scaled and its y prices\n",
    "\n",
    "#so now, we're gonna use that model to predict the prices of both the training set (as is customary) and the test set:\n",
    "y_tr_pred = lm.predict(X_tr_scaled)\n",
    "y_te_pred = lm.predict(X_te_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 43.22783554,  41.46645569,  59.97865097, 103.99172848,\n",
       "        57.56529339])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58.53661715, 54.58612029, 73.32003556, 40.20413174, 57.19754578])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_te_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.8.1.1.6 Assess model performance<a id='4.8.1.1.6_Assess_model_performance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay so now we'll see how these predictions did compared to the actuals using our r2 metric which tells us\n",
    "#how much the model/predictions improved the variance\n",
    "\n",
    "#predicted variance >>  predicted values vs actuals\n",
    "#------------------    ------------------------------\n",
    "#original variance  >>  actuals values vs their mean(right?)\n",
    "\n",
    "\n",
    "#aka, how do the predicted values compare to the actuals vs the actuals compared to each other?\n",
    "#if the predictions can get closer to / lasso / capture the actuals better than the actuals can group themselves / are\n",
    "#grouped/laid out naturally, then that'a a good sign / a potential keeper, a step in the right direction\n",
    "\n",
    "\n",
    "#REMEM, unlike the original way we did predictions, this time, the predictions for the test set are BASED on the test\n",
    "#set, NOT on the training. i mean, like last time, we tested the prediction of using the average training for the test\n",
    "#set. the lm model was formed using the scaled training data, but that's always what you do: model/fit/regress/learn on\n",
    "#train, apply/test on test\n",
    "#but yeah, here, train & test each got their own set of predictions, a unique value, rather than constant, for each\n",
    "#resort. but again, the WHOLE set, training + test, used the WHOLE/COMBINED data set's columns' medians to fill in\n",
    "#the nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8177988515690604, 0.7209725843435145)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# r^2 - train, test\n",
    "#this'll tell us the r2 on the basis of using the MEDIAN to fill in the nans!\n",
    "#remember ORDER MATTERS W R2!!!\n",
    "#must be actuals, predictions, and indeed this follows\n",
    "median_r2 = r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)\n",
    "median_r2\n",
    "\n",
    "\n",
    "#so what do we expect?\n",
    "#well, we'd expect, as always, that the prediction would be better/more accurate/less variation for the TRAINING SET\n",
    "#since that's what it's based on, compared to the test set\n",
    "\n",
    "#i mean it's like if you teach a kid what everything in the house is - stairs, door, window, banana, sink, lota, etc..\n",
    "#assumably they could also identify those things in ANY house, but also assumably, they'd identify them BETTER in their\n",
    "#OWN house!! the one they learned on! cuz it's a one-for-one match! no variation from their learning model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7209725843435145"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cool! so indeed the prediction is WAY better on the training set than the test. in fact, it eliminated 82% of the variance!!!\n",
    "#(means it's \"82% accurate?\")\n",
    "\n",
    "#but why is the second one so different???\n",
    "\n",
    "#hmm, i'm not sure. the template shows that it should be 0.72 - which is quite different than this\n",
    "\n",
    "#well let's double check and see what we get w/ our OWN homebrewed/homemade formula from scratch:\n",
    "\n",
    "r_squared(y_test, y_te_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hmm dang. well the good thing is that this is consistent w/ it - but the bad thing is we still don't know why this is\n",
    "# different than the template. either the template is wrong? or somehow i messed up one of these lists\n",
    "\n",
    "# I mean the formula only involves y test and y te pred. and we def didn't change y test. we defined that way in the\n",
    "# very beginning\n",
    "\n",
    "# looked and we def never altered y_test\n",
    "\n",
    "# so there's gotta be something w/ y_te_pred\n",
    "\n",
    "\n",
    "# even tried switching em but this isn't right either, although it's closer: r_squared(y_te_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ohhh. could it be that it has to do w how it randomly split the data into training and test? like maybe every time you\n",
    "#split it you get a diff split?\n",
    "#and isnt there like an id code you can use to assign it so you can get the same reproducible results every time?\n",
    "\n",
    "#yes! thats randomstate, and we DID use that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "summit_elev                           0\n",
       "vertical_drop                         0\n",
       "base_elev                             0\n",
       "trams                                 0\n",
       "fastSixes                             0\n",
       "fastQuads                             0\n",
       "quad                                  0\n",
       "triple                                0\n",
       "double                                0\n",
       "surface                               0\n",
       "total_chairs                          0\n",
       "Runs                                  0\n",
       "TerrainParks                          0\n",
       "LongestRun_mi                         0\n",
       "SkiableTerrain_ac                     0\n",
       "Snow Making_ac                        0\n",
       "daysOpenLastYear                      0\n",
       "yearsOpen                             0\n",
       "averageSnowfall                       0\n",
       "projectedDaysOpen                     0\n",
       "NightSkiing_ac                        0\n",
       "Total_Resorts                         0\n",
       "resorts_per_100kcapita                0\n",
       "resorts_per_100ksq_mile               0\n",
       "resort_skiable_area_ac_state_ratio    0\n",
       "resort_days_open_state_ratio          0\n",
       "resort_terrain_park_state_ratio       0\n",
       "resort_night_skiing_state_ratio       0\n",
       "total_chairs_runs_ratio               0\n",
       "total_chairs_skiable_ratio            0\n",
       "fastQuads_runs_ratio                  0\n",
       "fastQuads_skiable_ratio               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.isna().sum()\n",
    "X_te.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just wanted to confirm. these all look good, for both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that you estimated ticket price by simply using a known average. As expected, this produced an $R^2$ of zero for both the training and test set, because $R^2$ tells us how much of the variance you're explaining beyond that of using just the mean, and you were using just the mean. Here we see that our simple linear regression model explains over 80% of the variance on the train set and over 70% on the test set. Clearly you are onto something, although the much lower value for the test set suggests you're overfitting somewhat. This isn't a surprise as you've made no effort to select a parsimonious set of features or deal with multicollinearity in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interesting. so yeah, we saw that when we simply used the mean as the predictor. using the mean gets you an r2 of 0 /\n",
    "#ZERO / JEERO because that's the BASELINE!! remem, that's like the break-even point, means that it just got you WHAT\n",
    "#YOU ALREADY HAD!!!\n",
    "\n",
    "#so yeah, that's an interesting way to look at it. r2 tells you how much of the variance you closed BEYOND just using\n",
    "#the mean, but it also just tells you how much you've closed/improved the variance from the original AT ALL!\n",
    "\n",
    "#ahh man, there it is again, they're saying the test r2 SHOULD be over 70%!! so how is mine so way off!? i guess\n",
    "#the randomizer thing doesn't explain it, which we already knew cuz of random state\n",
    "\n",
    "#but that's also interesting that they say the 70% is 'MUCH' lower than the 80%??\n",
    "\n",
    "#and parsimonious? like economical? or i guess carefully selected\n",
    "\n",
    "#lets see if my other numbers don't line up w/ the template..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.547850301825427, 9.40702011858132)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code task 12#\n",
    "#Now calculate the mean absolute error scores using `sklearn`'s `mean_absolute_error` function\n",
    "# as we did above for R^2\n",
    "# MAE - train, test\n",
    "median_mae = mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)\n",
    "median_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dang, they don't tell you what these are supposed to be - wasn't run in template. but from below looks like we're\n",
    "#about right, in the ballpark. although the $19 was based off the TEST mae. here our test mae is closer to $12. it's our\n",
    "#TRAINING set that's close to $9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#but yeah, that's a good reminder on what mae is. it's the average ABSOLUTE (i.e. NON-squared) diff between\n",
    "#the predictions and the actuals, like a margin of error!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this model, then, on average you'd expect to estimate a ticket price within \\\\$9 or so of the real price. This is much, much better than the \\\\$19 from just guessing using the average. There may be something to this machine learning lark after all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111.89581253658478, 161.7315645119227)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code task 13#\n",
    "#And also do the same using `sklearn`'s `mean_squared_error`\n",
    "# MSE - train, test\n",
    "median_mse = mean_squared_error(y_tr_pred, y_train), mean_squared_error(y_te_pred, y_test)\n",
    "median_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.8.1.2 Impute missing values with the mean<a id='4.8.1.2_Impute_missing_values_with_the_mean'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You chose to use the median for filling missing values because of the skew of many of our predictor feature distributions. What if you wanted to try something else, such as the mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.8.1.2.1 Learn the values to impute from the train set<a id='4.8.1.2.1_Learn_the_values_to_impute_from_the_train_set'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "summit_elev                           4074.554404\n",
       "vertical_drop                         1043.196891\n",
       "base_elev                             3020.512953\n",
       "trams                                    0.103627\n",
       "fastSixes                                0.072539\n",
       "fastQuads                                0.673575\n",
       "quad                                     1.010363\n",
       "triple                                   1.440415\n",
       "double                                   1.813472\n",
       "surface                                  2.497409\n",
       "total_chairs                             7.611399\n",
       "Runs                                    41.188482\n",
       "TerrainParks                             2.434783\n",
       "LongestRun_mi                            1.293122\n",
       "SkiableTerrain_ac                      448.785340\n",
       "Snow Making_ac                         129.601190\n",
       "daysOpenLastYear                       110.100629\n",
       "yearsOpen                               56.559585\n",
       "averageSnowfall                        162.310160\n",
       "projectedDaysOpen                      115.920245\n",
       "NightSkiing_ac                          86.384615\n",
       "Total_Resorts                           16.264249\n",
       "resorts_per_100kcapita                   0.424802\n",
       "resorts_per_100ksq_mile                 40.957785\n",
       "resort_skiable_area_ac_state_ratio       0.097205\n",
       "resort_days_open_state_ratio             0.126014\n",
       "resort_terrain_park_state_ratio          0.116022\n",
       "resort_night_skiing_state_ratio          0.155024\n",
       "total_chairs_runs_ratio                  0.271441\n",
       "total_chairs_skiable_ratio               0.070483\n",
       "fastQuads_runs_ratio                     0.010401\n",
       "fastQuads_skiable_ratio                  0.001633\n",
       "dtype: float64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code task 14#\n",
    "#As we did for the median above, calculate mean values for imputing missing values\n",
    "# These are the values we'll use to fill in any missing values\n",
    "X_defaults_mean = X_train.mean()\n",
    "X_defaults_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so again we're using only the TRAINING's mean and will use these to apply to the WHOLE DATA'S blanks!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By eye, you can immediately tell that your replacement values are much higher than those from using the median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.8.1.2.2 Apply the imputation to both train and test splits<a id='4.8.1.2.2_Apply_the_imputation_to_both_train_and_test_splits'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the new, backfilled columns, using mean\n",
    "X_tr = X_train.fillna(X_defaults_mean)\n",
    "X_te = X_test.fillna(X_defaults_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.8.1.2.3 Scale the data<a id='4.8.1.2.3_Scale_the_data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the data down, mean-center it and transform it / cast / hang it onto / unto the new tilted / twisted axes,\n",
    "#the PCs\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tr)\n",
    "X_tr_scaled = scaler.transform(X_tr)\n",
    "X_te_scaled = scaler.transform(X_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.8.1.2.4 Train the model on the train split<a id='4.8.1.2.4_Train_the_model_on_the_train_split'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression().fit(X_tr_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.8.1.2.5 Make predictions using the model on both train and test splits<a id='4.8.1.2.5_Make_predictions_using_the_model_on_both_train_and_test_splits'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the prices based off the scaled/transformed X's\n",
    "\n",
    "y_tr_pred = lm.predict(X_tr_scaled)\n",
    "y_te_pred = lm.predict(X_te_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.8.1.2.6 Assess model performance<a id='4.8.1.2.6_Assess_model_performance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8170154093990025, 0.7163814716959958)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.536884040670973, 9.416375625789277)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112.37695054778276, 164.3926930952438)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results don't seem very different to when you used the median for imputing missing values. Perhaps it doesn't make much difference here. Maybe your overtraining dominates. Maybe other feature transformations, such as taking the log, would help. You could try with just a subset of features rather than using all of them as inputs.\n",
    "\n",
    "To perform the median/mean comparison, you copied and pasted a lot of code just to change the function for imputing missing values. It would make more sense to write a function that performed the sequence of steps:\n",
    "1. impute missing values\n",
    "2. scale the features\n",
    "3. train a model\n",
    "4. calculate model performance\n",
    "\n",
    "But these are common steps and `sklearn` provides something much better than writing custom functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8.2 Pipelines<a id='4.8.2_Pipelines'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important and useful components of `sklearn` is the [pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). In place of `panda`'s `fillna` DataFrame method, there is `sklearn`'s `SimpleImputer`. Remember the first linear model above performed the steps:\n",
    "\n",
    "1. replace missing values with the median for each feature\n",
    "2. scale the data to zero mean and unit variance\n",
    "3. train a linear regression model\n",
    "\n",
    "and all these steps were trained on the train split and then applied to the test split for assessment.\n",
    "\n",
    "The pipeline below defines exactly those same steps. Crucially, the resultant `Pipeline` object has a `fit()` method and a `predict()` method, just like the `LinearRegression()` object itself. Just as you might create a linear regression model and train it with `.fit()` and predict with `.predict()`, you can wrap the entire process of imputing and feature scaling and regression in a single object you can train with `.fit()` and predict with `.predict()`. And that's basically a pipeline: a model on steroids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.8.2.1 Define the pipeline<a id='4.8.2.1_Define_the_pipeline'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(\n",
    "    SimpleImputer(strategy='median'), \n",
    "    StandardScaler(), \n",
    "    LinearRegression()\n",
    ")\n",
    "\n",
    "#so this lets us set up / bundle all 3 of these steps into one step - using median method for nan backfill,\n",
    "#standardscaler for scaling/transforming, and LinearRegression for making the prediction / best-fit line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.pipeline.Pipeline"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(pipe, 'fit'), hasattr(pipe, 'predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.8.2.2 Fit the pipeline<a id='4.8.2.2_Fit_the_pipeline'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, a single call to the pipeline's `fit()` method combines the steps of learning the imputation (determining what values to use to fill the missing ones), the scaling (determining the mean to subtract and the variance to divide by), and then training the model. It does this all in the one call with the training data as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')),\n",
       "                ('standardscaler', StandardScaler()),\n",
       "                ('linearregression', LinearRegression())])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wow. what a function. does all 3 of these steps in 1!\n",
    "\n",
    "#Code task 15#\n",
    "#Call the pipe's `fit()` method with `X_train` and `y_train` as arguments\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.8.2.3 Make predictions on the train and test sets<a id='4.8.2.3_Make_predictions_on_the_train_and_test_sets'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a prediction of the training prices and test prices using the training X's and test X's\n",
    "\n",
    "y_tr_pred = pipe.predict(X_train)\n",
    "y_te_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.8.2.4 Assess performance<a id='4.8.2.4_Assess_performance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8177988515690604, 0.7209725843435145)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compare with your earlier (non-pipeline) result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8177988515690604, 0.7209725843435145)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nice! so it is the exact same, but the test r2 is still way off from what they had"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.547850301825427, 9.40702011858132)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with your earlier result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.547850301825427, 9.40702011858132)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#right on dude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111.89581253658478, 161.7315645119227)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with your earlier result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111.89581253658478, 161.7315645119227)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cowabunga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results confirm the pipeline is doing exactly what's expected, and results are identical to your earlier steps. This allows you to move faster but with confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9 Refining The Linear Model<a id='4.9_Refining_The_Linear_Model'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You suspected the model was overfitting. This is no real surprise given the number of features you blindly used. It's likely a judicious subset of features would generalize better. `sklearn` has a number of feature selection functions available. The one you'll use here is `SelectKBest` which, as you might guess, selects the k best features. You can read about SelectKBest \n",
    "[here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest). `f_regression` is just the [score function](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html#sklearn.feature_selection.f_regression) you're using because you're performing regression. It's important to choose an appropriate one for your machine learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ahh okay i understand. overfitting is when you're getting TOO specific on what makes something something\n",
    "\n",
    "#like when training a computer or a kid to recognize a cat - don't get so hung up on things like the color or size\n",
    "#focus on the shape and features like whiskers, ears, tail ears etc\n",
    "\n",
    "#THAT'S HOW YOU ***OVER***FIT!!!\n",
    "\n",
    "#you gotta keep it loose enough so that you're recognizing PATTERNS, NOT SPECIFICS!!! wanna be able to identify FAMILIES!\n",
    "#NOT individuals!!! like be able to figure out what a cat is, not which one CAGO is!\n",
    "\n",
    "\n",
    "#SelectKBest is a sklearn function that helps you select / selects the K best features for you\n",
    "\n",
    "#f_regression? score function??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.1 Define the pipeline<a id='4.9.1_Define_the_pipeline'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefine your pipeline to include this feature selection step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this time we're gonna narrow in and focus only on the features that matter most\n",
    "\n",
    "#Code task 16#\n",
    "#Add `SelectKBest` as a step in the pipeline between `StandardScaler()` and `LinearRegression()`\n",
    "#Don't forget to tell it to use `f_regression` as its score function\n",
    "pipe = make_pipeline(\n",
    "    SimpleImputer(strategy='median'), \n",
    "    StandardScaler(),\n",
    "    SelectKBest(score_func=f_regression),\n",
    "    LinearRegression()\n",
    ")\n",
    "\n",
    "#quickly, f_regression has to do w/ correlations between regressors/positive predictors and gives p-values\n",
    "#ranks features in order (of correlation?) based on F-scores, which are converted to p-values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.2 Fit the pipeline<a id='4.9.2_Fit_the_pipeline'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')),\n",
       "                ('standardscaler', StandardScaler()),\n",
       "                ('selectkbest',\n",
       "                 SelectKBest(score_func=<function f_regression at 0x7f8a30c49d30>)),\n",
       "                ('linearregression', LinearRegression())])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this confirms all the functions / ingredients you've baked in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.3 Assess performance on the train and test set<a id='4.9.3_Assess_performance_on_the_train_and_test_set'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr_pred = pipe.predict(X_train)\n",
    "y_te_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7674914326052744, 0.6259877354190837)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yeah my test numbers continue to be wayyy off, but training is spot on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.501495079727484, 11.201830190332052)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has made things worse! Clearly selecting a subset of features has an impact on performance. `SelectKBest` defaults to k=10. You've just seen that 10 is worse than using all features. What is the best k? You could create a new pipeline with a different value of k:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#haha wow, interesting, we were better off using ALL features than the top 10?? how does that work??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.4 Define a new pipeline to select a different number of features<a id='4.9.4_Define_a_new_pipeline_to_select_a_different_number_of_features'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code task 17#\n",
    "#Modify the `SelectKBest` step to use a value of 15 for k\n",
    "pipe15 = make_pipeline(\n",
    "    SimpleImputer(strategy='median'), \n",
    "    StandardScaler(),\n",
    "    SelectKBest(score_func=f_regression, k=15),\n",
    "    LinearRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.5 Fit the pipeline<a id='4.9.5_Fit_the_pipeline'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')),\n",
       "                ('standardscaler', StandardScaler()),\n",
       "                ('selectkbest',\n",
       "                 SelectKBest(k=15,\n",
       "                             score_func=<function f_regression at 0x7f8a30c49d30>)),\n",
       "                ('linearregression', LinearRegression())])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe15.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.6 Assess performance on train and test data<a id='4.9.6_Assess_performance_on_train_and_test_data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr_pred = pipe15.predict(X_train)\n",
    "y_te_pred = pipe15.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7924096060483825, 0.6376199973170793)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.211767769307114, 10.48824686729436)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could keep going, trying different values of k, training a model, measuring performance on the test set, and then picking the model with the best test set performance. There's a fundamental problem with this approach: _you're tuning the model to the arbitrary test set_! If you continue this way you'll end up with a model works well on the particular quirks of our test set _but fails to generalize to new data_. The whole point of keeping a test set is for it to be a set of that new data, to check how well our model might perform on data it hasn't seen.\n",
    "\n",
    "The way around this is a technique called _cross-validation_. You partition the training set into k folds, train our model on k-1 of those folds, and calculate performance on the fold not used in training. This procedure then cycles through k times with a different fold held back each time. Thus you end up building k models on k sets of data with k estimates of how the model performs on unseen data but without having to touch the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ahh yes, so just as we don't want to OVERfit too specifically to the training set, we also don't wanna keep finetuning\n",
    "#so specifically / to master just the ONE test set!! cuz we need it to be general and work across the board, look for\n",
    "#TRENDS/PATTERNS!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.7 Assessing performance using cross-validation<a id='4.9.7_Assessing_performance_using_cross-validation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(pipe15, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.63760862, 0.72831381, 0.74443537, 0.5487915 , 0.50441472])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores = cv_results['test_score']\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are r2's right???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so this means it re-runs the code 5 times because, w/o a randomstater, it'll return a different result each time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without using the same random state for initializing the CV folds, your actual numbers will be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6327128053007862, 0.09502487849877704)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cv_scores), np.std(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results highlight that assessing model performance in inherently open to variability. You'll get different results depending on the quirks of which points are in which fold. An advantage of this is that you can also obtain an estimate of the variability, or uncertainty, in your performance estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ahh okay true, so you can't just base your model off of what works w/ ONE split! then if you / as you tweak you're\n",
    "#gonna over fit too specific. so you gotta keep shaking it up and trying again and seeing how it compares and how\n",
    "#close/consistent/accurate/precise the results are, like sighting your scope / archery!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44, 0.82])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round((np.mean(cv_scores) - 2 * np.std(cv_scores), np.mean(cv_scores) + 2 * np.std(cv_scores)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hain - so this is the average cv score - 2 times the cv scores' stdv, and 2 ..... oh\n",
    "#it's showing the range of scores to 4 (+/-2 from the mean) standard deviations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.8 Hyperparameter search using GridSearchCV<a id='4.9.8_Hyperparameter_search_using_GridSearchCV'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pulling the above together, we have:\n",
    "* a pipeline that\n",
    "    * imputes missing values\n",
    "    * scales the data\n",
    "    * selects the k best features\n",
    "    * trains a linear regression model\n",
    "* a technique (cross-validation) for estimating model performance\n",
    "\n",
    "Now you want to use cross-validation for multiple values of k and use cross-validation to pick the value of k that gives the best performance. `make_pipeline` automatically names each step as the lowercase name of the step and the parameters of the step are then accessed by appending a double underscore followed by the parameter name. You know the name of the step will be 'selectkbest' and you know the parameter is 'k'.\n",
    "\n",
    "You can also list the names of all the parameters in a pipeline like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'simpleimputer', 'standardscaler', 'selectkbest', 'linearregression', 'simpleimputer__add_indicator', 'simpleimputer__copy', 'simpleimputer__fill_value', 'simpleimputer__missing_values', 'simpleimputer__strategy', 'simpleimputer__verbose', 'standardscaler__copy', 'standardscaler__with_mean', 'standardscaler__with_std', 'selectkbest__k', 'selectkbest__score_func', 'linearregression__copy_X', 'linearregression__fit_intercept', 'linearregression__n_jobs', 'linearregression__normalize', 'linearregression__positive'])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code task 18#\n",
    "#Call `pipe`'s `get_params()` method to get a dict of available parameters and print their names\n",
    "#using dict's `keys()` method\n",
    "pipe.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above can be particularly useful as your pipelines becomes more complex (you can even nest pipelines within pipelines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"k will be equal to k+1 in the range from 0 to the number of the number of columns in the X_training set\"\n",
    "\n",
    "k = [k+1 for k in range(len(X_train.columns))]\n",
    "\n",
    "# then this is a new variable that is a key:value pair for the dictionary that's calling on its selectkbest__k feature\n",
    "# and will impute the k in accordance to the definition above?\n",
    "grid_params = {'selectkbest__k': k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 32)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(len(X_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a range of `k` to investigate. Is 1 feature best? 2? 3? 4? All of them? You could write a for loop and iterate over each possible value, doing all the housekeeping ourselves to track the best value of k. But this is  a common task so there's a built in function in `sklearn`. This is [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "This takes the pipeline object, in fact it takes anything with a `.fit()` and `.predict()` method. In simple cases with no feature selection or imputation or feature scaling etc. you may see the classifier or regressor object itself directly passed into `GridSearchCV`. The other key input is the parameters and values to search over. Optional parameters include the cross-validation strategy and number of CPUs to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_grid_cv = GridSearchCV(pipe, param_grid=grid_params, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('simpleimputer',\n",
       "                                        SimpleImputer(strategy='median')),\n",
       "                                       ('standardscaler', StandardScaler()),\n",
       "                                       ('selectkbest',\n",
       "                                        SelectKBest(score_func=<function f_regression at 0x7f8a30c49d30>)),\n",
       "                                       ('linearregression',\n",
       "                                        LinearRegression())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'selectkbest__k': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,\n",
       "                                            12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
       "                                            21, 22, 23, 24, 25, 26, 27, 28, 29,\n",
       "                                            30, ...]})"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_grid_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay so this is gonna like iterate thru each possible k value, from 1 (cuz k + 1), to 33 technically, cuz will follow\n",
    "#that def, but is no 33rd, so that'll be null; so in reality will go up to 32. we do the +1 cuz the counter starts at\n",
    "#0 and we don't want a k of 0 of course cuz that'd be no good! we're basically converting / translating indexing\n",
    "#language to usable math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create these new columns to add to our table\n",
    "\n",
    "score_mean = lr_grid_cv.cv_results_['mean_test_score']\n",
    "score_std = lr_grid_cv.cv_results_['std_test_score']\n",
    "cv_k = [k for k in lr_grid_cv.cv_results_['param_selectkbest__k']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'selectkbest__k': 8}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code task 19#\n",
    "#Print the `best_params_` attribute of `lr_grid_cv`\n",
    "lr_grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oh shoot, ok, so it'll go thru all of em and tell you which is the best number of features to focus on, the most\n",
    "#influential. does it tell you which ones those are tho?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Code task 20#\n",
    "#Assign the value of k from the above dict of `best_params_` and assign it to `best_k`\n",
    "\n",
    "#so best_k will hold the value above we just saw: lr_grid_cv.best_params_ >> {'selectkbest__k': 8}\n",
    "best_k = lr_grid_cv.best_params_['selectkbest__k'] #why do we need to write out ['selectkbest__k']? don't we already get\n",
    "#that when we call just the first part before that?\n",
    "#ohhh wait ok i get it. we want JUST the number (8), so we call it on this end so it returns JUST a/the number!\n",
    "\n",
    "\n",
    "#setting up / staging all the graphing/plotting elements\n",
    "\n",
    "plt.subplots(figsize=(10, 5))\n",
    "plt.errorbar(cv_k, score_mean, yerr=score_std)\n",
    "plt.axvline(x=best_k, c='r', ls='--', alpha=.5)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('CV score (r-squared)')\n",
    "plt.title('Pipeline mean CV score (error bars +/- 1sd)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oh wow very cool! so it tells us / shows us which is our optimal k - the one w/ the lowest cv score / aka r2\n",
    "#but wait...... isn't the HIGHER the r2 score the BETTER!?! cuz that's telling how much your variance improved/decreased....\n",
    "#oh wait wow nevermind - yes that's exactly right - I was looking at the size / height of the ERROR BARS and not the\n",
    "#height of the point on the line. so yes this makes perfect sense. @k=8 the mean cv'd r2 square is highest and also\n",
    "#the error is smallest? i thought maybe those would be correlated but not necessarily? i mean maybe the highest r2 also\n",
    "#has the lowest error (like mae), but if you look, you can clearly see that the lowest values don't necessarily have\n",
    "#the least error. it seems more like, the more features you try to use/the higher the k, the more the error! although\n",
    "#it seems to get lower until you hit 8 which is again the sweet spot and then climbs back up\n",
    "\n",
    "#also, as noted below, although the error seems to increase by alot, the r2 doesn't seem to go down by too much after\n",
    "#k=8, but it is a rapid ascent to it\n",
    "\n",
    "#ohh ok, so i think i understand the syntax now. the error bar represents the STANDARD DEVIATION of the cv scores @\n",
    "#that k number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above suggests a good value for k is 8. There was an initial rapid increase with k, followed by a slow decline. Also noticeable is the variance of the results greatly increase above k=8. As you increasingly overfit, expect greater swings in performance as different points move in and out of the train/test folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ahh okay, yeah makes alot of sense! if you get too specific / custom, your cv scores when you randomize w/ each sample\n",
    "#are gonna swing ALOT due to the presence and absence of certain members!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which features were most useful? Step into your best model, shown below. Starting with the fitted grid search object, you get the best estimator, then the named step 'selectkbest', for which you can call its `get_support()` method for a logical mask of the features selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so what does this mean? so lr_grid_cv is the big package function we packed w/ features & methods\n",
    "#best_estimator - not sure what that is? we haven't used that yet? we've seen best_PARAMS\n",
    "\n",
    "#why do we gotta specify \"named_steps\"? why can't we just.... name it? to get to it\n",
    "\n",
    "selected = lr_grid_cv.best_estimator_.named_steps.selectkbest.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False,  True, False,  True, False, False, False,\n",
       "       False,  True,  True, False,  True,  True,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ok so there's 32 so this is one for each column.... but what is it true/falsing?\n",
    "#ohhh - like whether it's one of the best / k true falsers??\n",
    "#well then this should sum up to 8!!\n",
    "selected.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Region', 'summit_elev', 'base_elev', 'triple', 'double',\n",
       "       'total_chairs', 'Runs', 'TerrainParks'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#yess!! sensational!\n",
    "\n",
    "#so we have the column names, so would we be able to just do this what they correlate to?\n",
    "\n",
    "#is it in the order of the original ski_data?\n",
    "\n",
    "#lets see what this gives:\n",
    "\n",
    "ski_data.columns[[1,3,5,10,11,13,14,15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nope,... that's not it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, instead of using the 'selectkbest' named step, you can access the named step for the linear regression model and, from that, grab the model coefficients via its `coef_` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oh so this would be like the equivalent of the 'components_' method for PCA! so this way we can see the ranks of the\n",
    "#features for each PC!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vertical_drop        10.767857\n",
       "Snow Making_ac        6.290074\n",
       "total_chairs          5.794156\n",
       "fastQuads             5.745626\n",
       "Runs                  5.370555\n",
       "LongestRun_mi         0.181814\n",
       "trams                -4.142024\n",
       "SkiableTerrain_ac    -5.249780\n",
       "dtype: float64"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code task 21#\n",
    "#Get the linear model coefficients from the `coef_` attribute and store in `coefs`,\n",
    "#get the matching feature names from the column names of the dataframe,\n",
    "#and display the results as a pandas Series with `coefs` as the values and `features` as the index,\n",
    "#sorting the values in descending order\n",
    "\n",
    "#so remember, this is the alternative to what we just did, except this will show us actual SCORES / relative standings\n",
    "#and magnitude\n",
    "coefs = lr_grid_cv.best_estimator_.named_steps.linearregression.coef_\n",
    "features = X_train.columns[selected]\n",
    "\n",
    "#ahh okay, i get it, yeah just exactly like what we did w/ pca.components_\n",
    "#turn it into a table by tacking/slapping on the column labels\n",
    "#cuz it doesn't rank the coeffs any particular way. just lays it out like a matrix\n",
    "#so it should be 32x32 right? cuz 32 components (rows) with 32 features each (across/columns)\n",
    "\n",
    "#oh wait, no. apparently not. i guess it just sums it up and like OVERALL, these are like the weights/scores/loadings?\n",
    "\n",
    "#like if you JUST chose the first principle component, these would be the coefficients of each feature in the equation\n",
    "#to predict y\n",
    "\n",
    "\n",
    "pd.Series(coefs, index=features).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#very interesting - so this does line up pretty closely with using simple correlation coefficients that we did in eda:\n",
    "#in order it was:\n",
    "\n",
    "#runs\n",
    "#fastquads\n",
    "#vertical drop\n",
    "#snow making acres\n",
    "#total chairs\n",
    "#days open\n",
    "#longest run\n",
    "#trams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and this is addressing something different than PCA because w/ PCA we weren't looking at price! we were seeing how\n",
    "#other factors were related to each other. but here, we're seeing how MANY factors are all correlated to SPECIFICALLY\n",
    "#PRICE!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='vertical_drop', ylabel='AdultWeekend'>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#wow so interesting! so vertical_drop is the think most closely tied to price??\n",
    "#so, as in, the price should be proportional to the vertical drop?\n",
    "\n",
    "sns.scatterplot(data=ski_data, x='vertical_drop', y='AdultWeekend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7130674701666047"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wow. look at that. there you have it. a very clear link. let's see the number:\n",
    "ski_data.vertical_drop.corr(ski_data.AdultWeekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prit-tee strong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results suggest that vertical drop is your biggest positive feature. This makes intuitive sense and is consistent with what you saw during the EDA work. Also, you see the area covered by snow making equipment is a strong positive as well. People like guaranteed skiing! The skiable terrain area is negatively associated with ticket price! This seems odd. People will pay less for larger resorts? There could be all manner of reasons for this. It could be  an effect whereby larger resorts can host more visitors at any one time and so can charge less per ticket. As has been mentioned previously, the data are missing information about visitor numbers. Bear in mind,  the coefficient for skiable terrain is negative _for this model_. For example, if you kept the total number of chairs and fastQuads constant, but increased the skiable terrain extent, you might imagine the resort is worse off because the chairlift capacity is stretched thinner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interesting - so you never know which way these things can go till you see the data, you can only conjecture\n",
    "#cuz like w/ supply, demand and price, there's always different possibilities that can explain a situation\n",
    "#like could be that price is low when demand is low or maybe demand is high but competition/supply is high too\n",
    "\n",
    "#so in the case of skiable acres, that's negatively correlated w/ price. on the outset, you might've thought that\n",
    "#parks with more land could/would charge more for a ticket because they're offering so much space. but in reality,\n",
    "#BECAUSE they're so open, there's not as much scarcity, thus they can't really charge more because there's not really\n",
    "#a 'competition' of ppl tryna get in. that's why companies/industry create 'artificial'/'planned' scarcity of supply\n",
    "#so that they can drive their own prices up. pretend there's no resources, but really they're purposely holding out so\n",
    "#they can charge more and make ppl fight/compete for / bid for stuff and give stuff only to those willing to pay the\n",
    "#most / to the highest bidder\n",
    "\n",
    "#trying to understand what the last bit is saying - if a resort held the number of chairs and fastquads constant,\n",
    "#but had more skiable terrain, then that might not work so well because it'd be harder for people to reach these areas\n",
    "#now ppl are gonna be fighting for chairs or there'll be unused/underutilized space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.10 Random Forest Model<a id='4.10_Random_Forest_Model'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model that can work very well in a lot of cases is the random forest. For regression, this is provided by `sklearn`'s `RandomForestRegressor` class.\n",
    "\n",
    "Time to stop the bad practice of repeatedly checking performance on the test split. Instead, go straight from defining the pipeline to assessing performance using cross-validation. `cross_validate` will perform the fitting as part of the process. This uses the default settings for the random forest so you'll then proceed to investigate some different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as we've seen w/ these, the common theme/thread is MASH UP AS MANY STEPS AS YOU CAN INTO ONE SUPERPACKAGE!\n",
    "#hashtagefficiency #bangforyourbuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10.1 Define the pipeline<a id='4.10.1_Define_the_pipeline'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code task 22#\n",
    "#Define a pipeline comprising the steps:\n",
    "#SimpleImputer() with a strategy of 'median'\n",
    "#StandardScaler(),\n",
    "#and then RandomForestRegressor() with a random state of 47\n",
    "RF_pipe = make_pipeline(\n",
    "    SimpleImputer(strategy='median'),\n",
    "    StandardScaler(),\n",
    "    RandomForestRegressor(random_state=47)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10.2 Fit and assess performance using cross-validation<a id='4.10.2_Fit_and_assess_performance_using_cross-validation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code task 23#\n",
    "#Call `cross_validate` to estimate the pipeline's performance.\n",
    "#Pass it the random forest pipe object, `X_train` and `y_train`,\n",
    "#and get it to use 5-fold cross-validation\n",
    "rf_default_cv_results = cross_validate(RF_pipe, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69249204, 0.78061953, 0.77546915, 0.62190924, 0.61742339])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cv_scores = rf_default_cv_results['test_score']\n",
    "rf_cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very close to template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6975826707112506, 0.07090742940774528)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rf_cv_scores), np.std(rf_cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so what exactly is random forest anyway??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10.3 Hyperparameter search using GridSearchCV<a id='4.10.3_Hyperparameter_search_using_GridSearchCV'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest has a number of hyperparameters that can be explored, however here you'll limit yourselves to exploring some different values for the number of trees. You'll try it with and without feature scaling, and try both the mean and median as strategies for imputing missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforestregressor__n_estimators': [10,\n",
       "  12,\n",
       "  16,\n",
       "  20,\n",
       "  26,\n",
       "  33,\n",
       "  42,\n",
       "  54,\n",
       "  69,\n",
       "  88,\n",
       "  112,\n",
       "  143,\n",
       "  183,\n",
       "  233,\n",
       "  297,\n",
       "  379,\n",
       "  483,\n",
       "  615,\n",
       "  784,\n",
       "  1000],\n",
       " 'standardscaler': [StandardScaler(), None],\n",
       " 'simpleimputer__strategy': ['mean', 'median']}"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_est = [int(n) for n in np.logspace(start=1, stop=3, num=20)]\n",
    "grid_params = {\n",
    "        'randomforestregressor__n_estimators': n_est,\n",
    "        'standardscaler': [StandardScaler(), None],\n",
    "        'simpleimputer__strategy': ['mean', 'median']\n",
    "}\n",
    "grid_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hain? what does this mean? i mean i see that we chose 20 numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code task 24#\n",
    "#Call `GridSearchCV` with the random forest pipeline, passing in the above `grid_params`\n",
    "#dict for parameters to evaluate, 5-fold cross-validation, and all available CPU cores (if desired)\n",
    "rf_grid_cv = GridSearchCV(RF_pipe, param_grid=grid_params, cv=5, n_jobs=-1)\n",
    "\n",
    "#what is it about the number of cpu cores to use? and what's n_jobs & -1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we made the GridSearchCV thing/pipeline/superpackage into an object and can now call on that to fit it w/ this\n",
    "#/the specific data that we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('simpleimputer',\n",
       "                                        SimpleImputer(strategy='median')),\n",
       "                                       ('standardscaler', StandardScaler()),\n",
       "                                       ('randomforestregressor',\n",
       "                                        RandomForestRegressor(random_state=47))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'randomforestregressor__n_estimators': [10, 12, 16, 20,\n",
       "                                                                 26, 33, 42, 54,\n",
       "                                                                 69, 88, 112,\n",
       "                                                                 143, 183, 233,\n",
       "                                                                 297, 379, 483,\n",
       "                                                                 615, 784,\n",
       "                                                                 1000],\n",
       "                         'simpleimputer__strategy': ['mean', 'median'],\n",
       "                         'standardscaler': [StandardScaler(), None]})"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code task 25#\n",
    "#Now call the `GridSearchCV`'s `fit()` method with `X_train` and `y_train` as arguments\n",
    "#to actually start the grid search. This may take a minute or two.\n",
    "rf_grid_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforestregressor__n_estimators': 69,\n",
       " 'simpleimputer__strategy': 'median',\n",
       " 'standardscaler': None}"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code task 26#\n",
    "#Print the best params (`best_params_` attribute) from the grid search\n",
    "rf_grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay so what is n_estimators / that list of 20 nums? and so it's saying 69 is the best?\n",
    "\n",
    "#okay so apparently n_estimators is the number of 'trees' in the forest... not sure what that means, will have to\n",
    "#research/learn this stuff later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like imputing with the median helps, but scaling the features doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6951357 , 0.79430697, 0.77170917, 0.62254707, 0.66499334])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_best_cv_results = cross_validate(rf_grid_cv.best_estimator_, X_train, y_train, cv=5)\n",
    "rf_best_scores = rf_best_cv_results['test_score']\n",
    "rf_best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7097384501425082, 0.06451341966873386)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rf_best_scores), np.std(rf_best_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've marginally improved upon the default CV results. Random forest has many more hyperparameters you could tune, but we won't dive into that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Code task 27#\n",
    "#Plot a barplot of the random forest's feature importances,\n",
    "#assigning the `feature_importances_` attribute of \n",
    "#`rf_grid_cv.best_estimator_.named_steps.randomforestregressor` to the name `imps` to then\n",
    "#create a pandas Series object of the feature importances, with the index given by the\n",
    "#training data column names, sorting the values in descending order\n",
    "plt.subplots(figsize=(10, 5))\n",
    "imps = rf_grid_cv.best_estimator_.named_steps.randomforestregressor.feature_importances_\n",
    "rf_feat_imps = pd.Series(imps, index=X_train.columns).sort_values(ascending=False)\n",
    "rf_feat_imps.plot(kind='bar')\n",
    "plt.xlabel('features')\n",
    "plt.ylabel('importance')\n",
    "plt.title('Best random forest regressor feature importances');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastQuads                             0.257980\n",
       "Runs                                  0.251806\n",
       "Snow Making_ac                        0.112931\n",
       "vertical_drop                         0.090373\n",
       "SkiableTerrain_ac                     0.027905\n",
       "total_chairs                          0.017614\n",
       "projectedDaysOpen                     0.016451\n",
       "resort_days_open_state_ratio          0.015495\n",
       "total_chairs_runs_ratio               0.014558\n",
       "daysOpenLastYear                      0.014163\n",
       "quad                                  0.014069\n",
       "resorts_per_100kcapita                0.013541\n",
       "NightSkiing_ac                        0.013288\n",
       "resort_skiable_area_ac_state_ratio    0.013233\n",
       "LongestRun_mi                         0.012103\n",
       "yearsOpen                             0.011755\n",
       "fastQuads_runs_ratio                  0.010685\n",
       "total_chairs_skiable_ratio            0.010472\n",
       "fastQuads_skiable_ratio               0.008659\n",
       "fastSixes                             0.007922\n",
       "Total_Resorts                         0.007794\n",
       "summit_elev                           0.007558\n",
       "resort_night_skiing_state_ratio       0.007510\n",
       "base_elev                             0.006247\n",
       "double                                0.006201\n",
       "averageSnowfall                       0.005658\n",
       "resorts_per_100ksq_mile               0.005615\n",
       "surface                               0.004748\n",
       "triple                                0.004617\n",
       "resort_terrain_park_state_ratio       0.004247\n",
       "TerrainParks                          0.004031\n",
       "trams                                 0.000770\n",
       "dtype: float64"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_feat_imps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_feat_imps.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interesting! i suspected as much as soon as i saw the .25's and rest downhill and small that they add up to 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dope mashaAllah. this is pretty close too to the eda corrs:\n",
    "\n",
    "#runs\n",
    "#fastquads\n",
    "#vertical drop\n",
    "#snow making ac\n",
    "#total chairs\n",
    "#days open\n",
    "#longest run\n",
    "#trams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encouragingly, the dominant top four features are in common with your linear model:\n",
    "* fastQuads\n",
    "* Runs\n",
    "* Snow Making_ac\n",
    "* vertical_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear model meaning the above, (linear regression?), not eda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.11 Final Model Selection<a id='4.11_Final_Model_Selection'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to select your final model to use for further business modeling! It would be good to revisit the above model selection; there is undoubtedly more that could be done to explore possible hyperparameters.\n",
    "It would also be worthwhile to investigate removing the least useful features. Gathering or calculating, and storing, features adds business cost and dependencies, so if features genuinely are not needed they should be removed.\n",
    "Building a simpler model with fewer features can also have the advantage of being easier to sell (and/or explain) to stakeholders.\n",
    "Certainly there seem to be four strong features here and so a model using only those would probably work well.\n",
    "However, you want to explore some different scenarios where other features vary so keep the fuller \n",
    "model for now. \n",
    "The business is waiting for this model and you have something that you have confidence in to be much better than guessing with the average price.\n",
    "\n",
    "Or, rather, you have two \"somethings\". You built a best linear model and a best random forest model. You need to finally choose between them. You can calculate the mean absolute error using cross-validation. Although `cross-validate` defaults to the $R^2$ [metric for scoring](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring) regression, you can specify the mean absolute error as an alternative via\n",
    "the `scoring` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aaaaaammmmmeeeeeeeennn to using fewer features / making a simpler model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.11.1 Linear regression model performance<a id='4.11.1_Linear_regression_model_performance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'neg_mean_absolute_error' uses the (negative of) the mean absolute error    (why???)\n",
    "lr_neg_mae = cross_validate(lr_grid_cv.best_estimator_, X_train, y_train, \n",
    "                            scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.499032338015297, 1.622060897679967)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_mae_mean = np.mean(-1 * lr_neg_mae['test_score'])     #ummm why not just use the regular/non-negative abso valu\n",
    "lr_mae_std = np.std(-1 * lr_neg_mae['test_score'])       #if you were just gonna make it all positive anyway / mult\n",
    "lr_mae_mean, lr_mae_std                                  #by -1???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.793465668669326"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, lr_grid_cv.best_estimator_.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.11.2 Random forest regression model performance<a id='4.11.2_Random_forest_regression_model_performance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_neg_mae = cross_validate(rf_grid_cv.best_estimator_, X_train, y_train, \n",
    "                            scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.644639167595688, 1.3528565172191818)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_mae_mean = np.mean(-1 * rf_neg_mae['test_score'])\n",
    "rf_mae_std = np.std(-1 * rf_neg_mae['test_score'])\n",
    "rf_mae_mean, rf_mae_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.537730050637332"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, rf_grid_cv.best_estimator_.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.11.3 Conclusion<a id='4.11.3_Conclusion'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest model has a lower cross-validation mean absolute error by almost \\\\$1. It also exhibits less variability. Verifying performance on the test set produces performance consistent with the cross-validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ohh i see, that's what the point of was of the last line in each of the last two blocks - testing it on the test to\n",
    "#see how it fared compared to on the train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.12 Data quantity assessment<a id='4.12_Data_quantity_assessment'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you need to advise the business whether it needs to undertake further data collection. Would more data be useful? We're often led to believe more data is always good, but gathering data invariably has a cost associated with it. Assess this trade off by seeing how performance varies with differing data set sizes. The `learning_curve` function does this conveniently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so this is gonna tell us how performane aka r2 changes w/ differing sample sizes? so like see where the point of\n",
    "#diminishing returns is?\n",
    "\n",
    "fractions = [.2, .25, .3, .35, .4, .45, .5, .6, .75, .8, 1.0]\n",
    "train_size, train_scores, test_scores = learning_curve(pipe, X_train, y_train, train_sizes=fractions)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#so this is gonna let us see if the test scores went up w/ more training or not!\n",
    "\n",
    "plt.subplots(figsize=(10, 5))\n",
    "plt.errorbar(train_size, test_scores_mean, yerr=test_scores_std)\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('CV scores')\n",
    "plt.title('Cross-validation score as training set size increases');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXACTLY! there you have it - ***SHARPLY*** DIMINISHING RETURNS!! AFTER 40 SAMPLES IT DOESN'T MATTER! so yeah we got\n",
    "#more than enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that you seem to have plenty of data. There's an initial rapid improvement in model scores as one would expect, but it's essentially levelled off by around a sample size of 40-50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.13 Save best model object from pipeline<a id='4.13_Save_best_model_object_from_pipeline'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code task 28#\n",
    "#This may not be \"production grade ML deployment\" practice, but adding some basic\n",
    "#information to your saved models can save your bacon in development.\n",
    "#Just what version model have you just loaded to reuse? What version of `sklearn`\n",
    "#created it? When did you make it?\n",
    "#Assign the pandas version number (`pd.__version__`) to the `pandas_version` attribute,\n",
    "#the numpy version (`np.__version__`) to the `numpy_version` attribute,\n",
    "#the sklearn version (`sklearn_version`) to the `sklearn_version` attribute,\n",
    "#and the current datetime (`datetime.datetime.now()`) to the `build_datetime` attribute\n",
    "#Let's call this model version '1.0'\n",
    "best_model = rf_grid_cv.best_estimator_\n",
    "best_model.version = 1.0 #this is the thing they're saying to just call 1.0 right\n",
    "best_model.pandas_version = 'pd.__version__'\n",
    "best_model.numpy_version = 'np.__version__'\n",
    "best_model.sklearn_version = 'sklearn_version'\n",
    "best_model.X_columns = [col for col in X_train.columns] #is this not the same thing as just saying X_train.columns alone?\n",
    "best_model.build_datetime = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A file already exists with this name.\n",
      "\n",
      "Do you want to overwrite? (Y/N)Y\n",
      "Writing file.  \"../models/ski_resort_pricing_model.pkl\"\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "\n",
    "modelpath = '../models'   #ohhh cool! didn't know you could do that. that's how you create a new file to your directory\n",
    "save_file(best_model, 'ski_resort_pricing_model.pkl', modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.14 Summary<a id='4.14_Summary'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: 1** Write a summary of the work in this notebook. Capture the fact that you gained a baseline idea of performance by simply taking the average price and how well that did. Then highlight that you built a linear model and the features that found. Comment on the estimate of its performance from cross-validation and whether its performance on the test split was consistent with this estimate. Also highlight that a random forest regressor was tried, what preprocessing steps were found to be best, and again what its estimated performance via cross-validation was and whether its performance on the test set was consistent with that. State which model you have decided to use going forwards and why. This summary should provide a quick overview for someone wanting to know quickly why the given model was chosen for the next part of the business problem to help guide important business decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A: 1** The focus of this unit was to practice predicting the price by filling in the blanks using different strategies, such as mean and median. We learned about splitting the data into a training set and test set, 70/30 respectively. We explored common metrics by which to judge the effectiveness of a prediction model, such as r-squared, mean absolute error and mean squared error. We first used simple training average as our predictor for test prices before moving on to more advanced techniques.\n",
    "\n",
    "We first manually created our own functions, then learned about the built-in functions scikitlearn provides, and then took it a step further and utilized Pipelines that combined all the steps into one - imputing, scaling, training, and testing effectiveness/error, cross validation. Cross-validation was good to learn because it taught us about the importance of not over-fitting to one specific data set and not get caught up in the specifics but rather the generalities and the trends.\n",
    "\n",
    "For predicting, we looked at both linear regression and random forest. We learned the importance of trimming down the number of features to what's most important - in line with the point above about not overfitting. We were able to work it into our pipeline package a method that determined for us the k best number of features to focus on and also the point of diminishing returns as far as sample size to seek. We also looked at minimizing the error in our mean cross-validation r2.\n",
    "\n",
    "In both we were able to see which features were most weighty/influential in the scores. With linear regression we used the coefficients method to see what those most influential features were. With random forest we looked at playing around with the n_estimators, i.e. the number of trees, to see what was most optimal and which features had the best importance. The important/weighty features as per random forest were essentially the same as for linear regression, which were both very similar to what we saw in EDA when we used correlation: runs, fastquads, vertical drop, snow making acres - thus it was unanimous that these features are definitely worthy of special focus/consideration. These 4 features will likely be the ones we focus on in our price modeling. In the end, we topped it all off by saving our best random forest model and all the paramters and information we used in this build / analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
