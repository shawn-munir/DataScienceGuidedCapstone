{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Modeling<a id='5_Modeling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Contents<a id='5.1_Contents'></a>\n",
    "* [5 Modeling](#5_Modeling)\n",
    "  * [5.1 Contents](#5.1_Contents)\n",
    "  * [5.2 Introduction](#5.2_Introduction)\n",
    "  * [5.3 Imports](#5.3_Imports)\n",
    "  * [5.4 Load Model](#5.4_Load_Model)\n",
    "  * [5.5 Load Data](#5.5_Load_Data)\n",
    "  * [5.6 Refit Model On All Available Data (excluding Big Mountain)](#5.6_Refit_Model_On_All_Available_Data_(excluding_Big_Mountain))\n",
    "  * [5.7 Calculate Expected Big Mountain Ticket Price From The Model](#5.7_Calculate_Expected_Big_Mountain_Ticket_Price_From_The_Model)\n",
    "  * [5.8 Big Mountain Resort In Market Context](#5.8_Big_Mountain_Resort_In_Market_Context)\n",
    "    * [5.8.1 Ticket price](#5.8.1_Ticket_price)\n",
    "    * [5.8.2 Vertical drop](#5.8.2_Vertical_drop)\n",
    "    * [5.8.3 Snow making area](#5.8.3_Snow_making_area)\n",
    "    * [5.8.4 Total number of chairs](#5.8.4_Total_number_of_chairs)\n",
    "    * [5.8.5 Fast quads](#5.8.5_Fast_quads)\n",
    "    * [5.8.6 Runs](#5.8.6_Runs)\n",
    "    * [5.8.7 Longest run](#5.8.7_Longest_run)\n",
    "    * [5.8.8 Trams](#5.8.8_Trams)\n",
    "    * [5.8.9 Skiable terrain area](#5.8.9_Skiable_terrain_area)\n",
    "  * [5.9 Modeling scenarios](#5.9_Modeling_scenarios)\n",
    "    * [5.9.1 Scenario 1](#5.9.1_Scenario_1)\n",
    "    * [5.9.2 Scenario 2](#5.9.2_Scenario_2)\n",
    "    * [5.9.3 Scenario 3](#5.9.3_Scenario_3)\n",
    "    * [5.9.4 Scenario 4](#5.9.4_Scenario_4)\n",
    "  * [5.10 Summary](#5.10_Summary)\n",
    "  * [5.11 Further work](#5.11_Further_work)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Introduction<a id='5.2_Introduction'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we now take our model for ski resort ticket price and leverage it to gain some insights into what price Big Mountain's facilities might actually support as well as explore the sensitivity of changes to various resort parameters. Note that this relies on the implicit assumption that all other resorts are largely setting prices based on how much people value certain facilities. Essentially this assumes prices are set by a free market.\n",
    "\n",
    "We can now use our model to gain insight into what Big Mountain's ideal ticket price could/should be, and how that might change under various scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay cool so we're gonna see what Big Mountain in Montana can actually charge based on what others are doing and their\n",
    "#parameters. We're gonna \"explore the sensitivity of changes to arious resort parameters\" - so that doesn't mean we're\n",
    "#gonna like literally change the parameters and see what happens to the price, cuz obviously we don't know what would\n",
    "#happen. I guess it just means explore the trends / relationships b/w certain params and price, maybe some do have a\n",
    "#direct linear correlation, maybe others have no correlation, and maybe others have a non-linear correlation\n",
    "\n",
    "#and it's saying the assumptions are free market conditions, i.e. that the price is being determined by supply &\n",
    "#demand, i.e. by how much ppl value certain things (params)\n",
    "\n",
    "#and always remember, we can never have ALL the data, we can only get a snapshot/representation of it\n",
    "#and we can never know how ppl truly feel about something, even if there's data on it / they answer survey questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Imports<a id='5.3_Imports'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle  #?\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import __version__ as sklearn_version\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Load Model<a id='5.4_Load_Model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected model version doesn't match version loaded\n",
      "Warning: model created under different sklearn version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deens/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:324: UserWarning: Trying to unpickle estimator SimpleImputer from version 0.24.2 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/deens/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:324: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 0.24.2 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/deens/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:324: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 0.24.2 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/deens/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:324: UserWarning: Trying to unpickle estimator Pipeline from version 0.24.2 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# This isn't exactly production-grade, but a quick check for development\n",
    "# These checks can save some head-scratching in development when moving from\n",
    "# one python environment to another, for example\n",
    "\n",
    "expected_model_version = '1.0'  #what was this supposed to represent again?\n",
    "model_path = '../models/ski_resort_pricing_model.pkl'\n",
    "if os.path.exists(model_path):\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    if model.version != expected_model_version:\n",
    "        print(\"Expected model version doesn't match version loaded\")\n",
    "    if model.sklearn_version != sklearn_version:\n",
    "        print(\"Warning: model created under different sklearn version\")\n",
    "else:\n",
    "    print(\"Expected model not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay cool so these are just version checkers to warn someone in the beginning that something may be off / not be what\n",
    "# they expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Load Data<a id='5.5_Load_Data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ski_data = pd.read_csv('../data/ski_data_step3_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oh once again i gotta change it up to match what they're expecting:\n",
    "\n",
    "ski_data = ski_data[ski_data.yearsOpen < 1000]\n",
    "\n",
    "ski_data.loc[ski_data.Name=='Silverton Mountain', 'SkiableTerrain_ac'] = 1819  #crucial step that i'd forgotten!!! this single value alone was so extreme that it completely threw off the r2 score and i was scratching my head so hard wondering what the heck happened!!!! this came to light when i did the plot/histo of skiable acres and saw this crazy outlier and scrunched up graph way diff from the template instantly noticeable!\n",
    "\n",
    "ski_data.dropna(subset=['AdultWeekend'], inplace=True)\n",
    "\n",
    "ski_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_mountain = ski_data[ski_data.Name == 'Big Mountain Resort']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>124</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <td>Big Mountain Resort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Region</th>\n",
       "      <td>Montana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>Montana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summit_elev</th>\n",
       "      <td>6817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vertical_drop</th>\n",
       "      <td>2353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_elev</th>\n",
       "      <td>4464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trams</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastSixes</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastQuads</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quad</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>triple</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>double</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surface</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_chairs</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Runs</th>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TerrainParks</th>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LongestRun_mi</th>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SkiableTerrain_ac</th>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Snow Making_ac</th>\n",
       "      <td>600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daysOpenLastYear</th>\n",
       "      <td>123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yearsOpen</th>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>averageSnowfall</th>\n",
       "      <td>333.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdultWeekend</th>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>projectedDaysOpen</th>\n",
       "      <td>123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NightSkiing_ac</th>\n",
       "      <td>600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total_Resorts</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resorts_per_100kcapita</th>\n",
       "      <td>1.122778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resorts_per_100ksq_mile</th>\n",
       "      <td>8.161045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resort_skiable_area_ac_state_ratio</th>\n",
       "      <td>0.140121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resort_days_open_state_ratio</th>\n",
       "      <td>0.129338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resort_terrain_park_state_ratio</th>\n",
       "      <td>0.148148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resort_night_skiing_state_ratio</th>\n",
       "      <td>0.84507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_chairs_runs_ratio</th>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_chairs_skiable_ratio</th>\n",
       "      <td>0.004667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastQuads_runs_ratio</th>\n",
       "      <td>0.028571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastQuads_skiable_ratio</th>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    124\n",
       "Name                                Big Mountain Resort\n",
       "Region                                          Montana\n",
       "state                                           Montana\n",
       "summit_elev                                        6817\n",
       "vertical_drop                                      2353\n",
       "base_elev                                          4464\n",
       "trams                                                 0\n",
       "fastSixes                                             0\n",
       "fastQuads                                             3\n",
       "quad                                                  2\n",
       "triple                                                6\n",
       "double                                                0\n",
       "surface                                               3\n",
       "total_chairs                                         14\n",
       "Runs                                              105.0\n",
       "TerrainParks                                        4.0\n",
       "LongestRun_mi                                       3.3\n",
       "SkiableTerrain_ac                                3000.0\n",
       "Snow Making_ac                                    600.0\n",
       "daysOpenLastYear                                  123.0\n",
       "yearsOpen                                          72.0\n",
       "averageSnowfall                                   333.0\n",
       "AdultWeekend                                       81.0\n",
       "projectedDaysOpen                                 123.0\n",
       "NightSkiing_ac                                    600.0\n",
       "Total_Resorts                                        12\n",
       "resorts_per_100kcapita                         1.122778\n",
       "resorts_per_100ksq_mile                        8.161045\n",
       "resort_skiable_area_ac_state_ratio             0.140121\n",
       "resort_days_open_state_ratio                   0.129338\n",
       "resort_terrain_park_state_ratio                0.148148\n",
       "resort_night_skiing_state_ratio                 0.84507\n",
       "total_chairs_runs_ratio                        0.133333\n",
       "total_chairs_skiable_ratio                     0.004667\n",
       "fastQuads_runs_ratio                           0.028571\n",
       "fastQuads_skiable_ratio                           0.001"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_mountain.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there we go, aH now iA should be good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Refit Model On All Available Data (excluding Big Mountain)<a id='5.6_Refit_Model_On_All_Available_Data_(excluding_Big_Mountain)'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next step requires some careful thought. We want to refit the model using all available data. But should we include Big Mountain data? On the one hand, we are _not_ trying to estimate model performance on a previously unseen data sample, so theoretically including Big Mountain data should be fine. One might first think that including Big Mountain in the model training would, if anything, improve model performance in predicting Big Mountain's ticket price. But here's where our business context comes in. The motivation for this entire project is based on the sense that Big Mountain needs to adjust its pricing. One way to phrase this problem: we want to train a model to predict Big Mountain's ticket price based on data from _all the other_ resorts! We don't want Big Mountain's current price to bias this. We want to calculate a price based only on its competitors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interesting, so when we want to predict/set one or a specific group of values, then the question is - do we include\n",
    "#that/those in the TEST set, or do we leave that out and go to it independently/separately, after having worked on our\n",
    "#model and testing it and being happy with it, and only then applying it to the target?\n",
    "\n",
    "#cuz remember, the goal is to SET a fair price for Big Mountain.... ****NOT**** predict what it currently is!\n",
    "#we don't care that much what it currently is -- the whole point is to set it RIGHT in light of what others in the\n",
    "#industry are / what the free market is doing!\n",
    "\n",
    "#aH that's exactly what they wrote too, which I came up w/ aH before reading it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay so our X's are all the features, except price, and excluding Big Mountain\n",
    "#and our y's, the thing we're tryna predict, are the $prices ('AdultWeekend'), excluding of course Big Mountain's\n",
    "\n",
    "#okay so here's what 'model' is:\n",
    "\n",
    "# model_path = '../models/ski_resort_pricing_model.pkl'\n",
    "# if the model_path exists, it's set to equal 'model'\n",
    "\n",
    "# so:\n",
    "\n",
    "# model = model_path = 'ski_resort_pricing_model.pkl' >> which is our set of dev/developer params!\n",
    "# this is our 'best_model' which is 'rf_grid_cv.best_estimator_'\n",
    "#which was: GridSearchCV(RF_pipe, param_grid=grid_params, cv=5, n_jobs=-1)\n",
    "\n",
    "# so the X set/df is all the rows except Big Mountain, and all the columns for those except price...\n",
    "# but wait a minute - HOW DOES IT KNOW NOT TO INCLUDE THE PRICE COLUMN?!\n",
    "# like yes it's set to use the columns 'X_columns', but where does that come from and how does it know not to use\n",
    "# the price column???\n",
    "\n",
    "# oh, so remem in 'model', which we defined at the end of the last unit, we set the 'X_columns' param to equal the\n",
    "# columns of X_train, which we THEN/THERE defined to be all the columns of ski_data EXCEPT AdultWeekend!!!\n",
    "\n",
    "# and then the y set is of course just one column - the price$ - AdultWeekend, and for all the rows except Big Mountain!\n",
    "\n",
    "X = ski_data.loc[ski_data.Name != \"Big Mountain Resort\", model.X_columns]\n",
    "y = ski_data.loc[ski_data.Name != \"Big Mountain Resort\", 'AdultWeekend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(276, 276)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deens/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:388: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
      "  warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter min_impurity_split for estimator DecisionTreeRegressor(criterion='mse', max_features='auto'). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cr/9gwdd0wn43d9d1db4xfhyvpc0000gn/T/ipykernel_4426/2489992722.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    429\u001b[0m                 \u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_INT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m             trees = [\n\u001b[0m\u001b[1;32m    432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_more_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             trees = [\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_more_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             ]\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_base.py\u001b[0m in \u001b[0;36m_make_estimator\u001b[0;34m(self, append, random_state)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \"\"\"\n\u001b[1;32m    158\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_estimator_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_params\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;31m# TODO: Remove in v1.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mset_params\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    241\u001b[0m                     \u001b[0;34m\"Invalid parameter %s for estimator %s. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                     \u001b[0;34m\"Check the list of available parameters \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter min_impurity_split for estimator DecisionTreeRegressor(criterion='mse', max_features='auto'). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we run the model multiple times to see how the negative MAE fares across multiple trials\n",
    "\n",
    "#and again, 'model' is our latest, best, random forest model from the end of unit 4:\n",
    "# rf_grid_cv.best_estimator_\n",
    "#the best_estimator_ means store the combo of params that gave us the highest score, like in terms of prediction accuracy\n",
    "\n",
    "cv_results = cross_validate(model, X, y, scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)\n",
    "\n",
    "#this is like exactly what we did before, in end unit 4, no?:\n",
    "# rf_neg_mae = cross_validate(rf_grid_cv.best_estimator_, X_train, y_train, scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oh wait, but so now, this time, as opposed to last time, WE'RE NOT SPLITTING INTO TRAINING & TEST GROUPS!!!\n",
    "#cuz we've already done that! and that's why we have that best rf model stored!\n",
    "#so now, we're, satisfied w/ our model/best one yet/so far, and so thus applying it to / expanding it to train on /\n",
    "#training it on the ENTIRE dataset MINUS/EXCEPT OUR TARGET - BIG MOUNTAIN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#the function we made above includes a 'test_scores' column using the type of scoring we specified/requested, which was\n",
    "#neg MAE, and so that's what these are!\n",
    "cv_results['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#very close to template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_mean, mae_std = np.mean(-1 * cv_results['test_score']), np.std(-1 * cv_results['test_score'])\n",
    "mae_mean, mae_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#again, super duper close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers will inevitably be different to those in the previous step that used a different training data set. They should, however, be consistent. It's important to appreciate that estimates of model performance are subject to the noise and uncertainty of data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oh yeah! so the reason they're different from unit 4's is because this uses the ENTIRE dataset (minus Big M) as the\n",
    "#training set whereas that only used 70% (and was also minus BM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Calculate Expected Big Mountain Ticket Price From The Model<a id='5.7_Calculate_Expected_Big_Mountain_Ticket_Price_From_The_Model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay so now we're homing in on JUST BM\n",
    "#pinpoint just its row and taking as the X predictors all its columns/features except for of course price - the thing\n",
    "#we're tryna predict!\n",
    "\n",
    "#and then of course the y is that price iA we'll predict\n",
    "X_bm = ski_data.loc[ski_data.Name == \"Big Mountain Resort\", model.X_columns]\n",
    "y_bm = ski_data.loc[ski_data.Name == \"Big Mountain Resort\", 'AdultWeekend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bm.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bm.values.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ahh okay, so the above explains what is going on here / what these things mean. see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_bm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so we're storing as a variable / telling it to use our best rf model to predict the price using the X_bm we just defined\n",
    "#note, as learned above, using '.item()' is SIMPLY to nicely isolate the single number that's trapped/wrapped in an\n",
    "#array brackets and extra/neous info and cherry-pick out just the single number itself! for clean easy viewing\n",
    "bm_pred = model.predict(X_bm).item()\n",
    "\n",
    "#but how does it know what it's predicting? like how does it know to use all those like 32 params and use it to come\n",
    "#up w/ the target price number? like doesn't it have to know the relationships between the training set's same set of\n",
    "#these 32 variables and then their prices?\n",
    "\n",
    "\n",
    "#ohhh yeah, so that's already all baked deep into the model, if you look back / trace back its roots/history!\n",
    "\n",
    "#that's why it was so good to do that intense, packed building package work earlier, cuz now it's as simple as just\n",
    "#calling on one simple word/variable, representing all of it - MODEL!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there we go! that's the predicted price! i.e., based on what everyone else is doing given their offerings, this is\n",
    "#what Big Mountain should charge, i.e. the FAIR MARKET PRICE\n",
    "\n",
    "#by contrast, this is what they were charging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as seen above, all this is simply doing is isolating down to the number itself; don't get confused / overwhelmed;\n",
    "#nothing fancy is happening here\n",
    "\n",
    "y_bm = y_bm.values.item()\n",
    "y_bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wow! so subhanaAllah they were charging $15 too less!/UNDERcharging / selling themselves short by $15 per person!!!\n",
    "#that's almost 20% short/under!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'Big Mountain Resort modelled price is ${bm_pred:.2f}, actual price is ${y_bm:.2f}.')\n",
    "print(f'Even with the expected mean absolute error of ${mae_mean:.2f}, this suggests there is room for an increase.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our number here is slightly more than template, but again, it's gonna be slightly different w/ each run. we could\n",
    "#run it like 5 times and take the average, right?\n",
    "\n",
    "#cross_validate(model,X, cv=5)\n",
    "\n",
    "#....^No.... at least not like this. not sure how you'd do it\n",
    "\n",
    "\n",
    "######################################################################\n",
    "##########################???QUESTION???##############################\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and oh right, good point - to look at the MAE, which is about $10. so that's saying even if we're wrong and max out\n",
    "# that mean error, we're STILL a few bucks above where they're at!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result should be looked at optimistically and doubtfully! The validity of our model lies in the assumption that other resorts accurately set their prices according to what the market (the ticket-buying public) supports. The fact that our resort seems to be charging that much less than what's predicted suggests our resort might be undercharging. \n",
    "But if ours is mispricing itself, are others? It's reasonable to expect that some resorts will be \"overpriced\" and some \"underpriced.\" Or if resorts are pretty good at pricing strategies, it could be that our model is simply lacking some key data? Certainly we know nothing about operating costs, for example, and they would surely help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right, so this assumes that OTHER resorts are basing their prices on features and market norms. but if everyone did that,\n",
    "# #how could ANYONE set their prices, like who would go first, and HOW would they go first?? #headscratcher\n",
    "\n",
    "#oh yeah, good point - maybe other resorts are mispricing themselves too cuz they don't know / maybe didn't base it off\n",
    "#of what others were charging! or maybe only based it off of other SIMILAR groups. BASICALLY LOOKING AT ***COMPS***\n",
    "# / NEAREST NEIGHBORS / ***CLUSTERS*** thru PCA!!!! that's maybe how these algorithms work!!! how many bedrooms, bathrooms, *newness* /\n",
    "# wildcard, garage, yard size, neighborhood, etc etc. but like w/ those too, maybe they weren't basing it off of what\n",
    "#others were going for!\n",
    "\n",
    "\n",
    "#oh yeah, and that's a key point - about operating costs. of course this is a huge factor in any business's pricing\n",
    "#strategy and we know NOTHING about this for ANY of our "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 Big Mountain Resort In Market Context<a id='5.8_Big_Mountain_Resort_In_Market_Context'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features that came up as important in the modeling (not just our final, random forest model) included:\n",
    "* vertical_drop\n",
    "* Snow Making_ac\n",
    "* total_chairs\n",
    "* fastQuads\n",
    "* Runs\n",
    "* LongestRun_mi\n",
    "* trams\n",
    "* SkiableTerrain_ac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A handy glossary of skiing terms can be found on the [ski.com](https://www.ski.com/ski-glossary) site. Some potentially relevant contextual information is that vertical drop, although nominally the height difference from the summit to the base, is generally taken from the highest [_lift-served_](http://verticalfeet.com/) point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay so i think it's saying vertical drop is the distance b/w the highest and lowest 'lift-served' points,\n",
    "# and 'lift-served' simply means the points/verticalities at which the lift drops off at?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's often useful to define custom functions for visualizing data in meaningful ways. The function below takes a feature name as an input and plots a histogram of the values of that feature. It then marks where Big Mountain sits in the distribution by marking Big Mountain's value with a vertical line using `matplotlib`'s [axvline](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.axvline.html) function. It also performs a little cleaning up of missing values and adds descriptive labels and a title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hey they copied me! this is the same idea of what i did earlier on w/ my plots and tables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code task 1#\n",
    "#Add code to the `plot_compare` function that displays a vertical, dashed line\n",
    "#on the histogram to indicate Big Mountain's position in the distribution\n",
    "\n",
    "#Hint: plt.axvline() plots a vertical line, its position for 'feature1'\n",
    "\n",
    "#would be `big_mountain['feature1'].values, we'd like a red line, which can be\n",
    "\n",
    "#specified with c='r', a dashed linestyle is produced by ls='--', (#'ls' = 'line style!')\n",
    "\n",
    "#and it's nice to give it a slightly reduced alpha value, such as 0.8.\n",
    "\n",
    "#Don't forget to give it a useful label (e.g. 'Big Mountain') so it's listed in the legend\n",
    "\n",
    "\n",
    "def plot_compare(feat_name, description, state=None, figsize=(10, 5)):\n",
    "#so remember, a function is like a food making kit, and it bundles all the ingredients together in the beginning,\n",
    "#to lay it out for the recipe. so you're gonna hand it:  (explained below) the feature/column name, a description\n",
    "#of that feature, the specific state (like Pennsylvania), and the fig_size that'll be used for plotting\n",
    "\n",
    "\n",
    "#so this is gonna result in a histo of the feature for ALL resorts, including BM, and we're gonna mark it to see where\n",
    "#it lies\n",
    "    \n",
    "    \"\"\"Graphically compare distributions of features.\n",
    "    \n",
    "    Plot histogram of values for all resorts and reference line to mark\n",
    "    Big Mountain's position.\n",
    "    \n",
    "    Arguments:\n",
    "    feat_name - the feature column name in the data\n",
    "    description - text description of the feature\n",
    "    state - select a specific state (None for all states)\n",
    "    figsize - (optional) figure size\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.subplots(figsize=figsize)\n",
    "    \n",
    "    #okay so apparently there's this quirk w/ hist, described below, where it sometimes doesn't like NaNs\n",
    "    # quirk that hist sometimes objects to NaNs, sometimes doesn't\n",
    "    # filtering only for finite values tidies this up\n",
    "    \n",
    "    if state is None:\n",
    "        ski_x = ski_data[feat_name]  #so if you DON'T specify a single state, this will just filter the ski_data down \n",
    "    else:                            #to just the column of the one specified feature and include ALL resorts/states\n",
    "                                     #since you didn't filter so it won't subset any further\n",
    "        ski_x = ski_data.loc[ski_data.state == state, feat_name]  #by contrast, if you DO specify a state, it'll take\n",
    "    ski_x = ski_x[np.isfinite(ski_x)]                           #it a step further and subset not just for/to that column,\n",
    "                                                                #but for that specific state!\n",
    "    plt.hist(ski_x, bins=30)\n",
    "    plt.axvline(x=big_mountain[feat_name].values, c='r', ls='--', alpha=0.8, label='Big Mountain')\n",
    "    #so obvy this only applies if you're doing ALL resorts/states/not specifying or are doing JUST the state of Montana!\n",
    "    \n",
    "    #so we have to tell it which x do dash-the-dotted line vertical drawn on - so the x- point @which/=equal to Big Mountain's value of that specific/particular feature!\n",
    "    #cool so we're not literally telling it to find the point/plot of Big Mountain and mark/highlight that, we got\n",
    "    #creative, did a natural workaround - we said take the VALUE OF BIG MOUNTAIN AND SIMPLY MARK THAT VALUE BAR!!!\n",
    "    plt.xlabel(description)\n",
    "    plt.ylabel('frequency')\n",
    "    plt.title(description + ' distribution for resorts in market share')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8.1 Ticket price<a id='5.8.1_Ticket_price'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at where Big Mountain sits overall amongst all resorts for price and for just other resorts in Montana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#okay sick. slick. so now we're gonna put this function to use and see where BM stands w/ price:\n",
    "\n",
    "plot_compare('AdultWeekend', 'Adult weekend ticket price ($)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay cool so we can see it's a bit towards the higher end, althought there's def several one-off outliers that push\n",
    "# way even further upstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's see how JUST Montana looks like and how Big Mountain stacks there\n",
    "#btw MINE WAS WAY BETTER - I actually colored the BAR and was SPECIFIC TO BIG MOUNTAIN! plotted on that basis,\n",
    "#i.e. one plot for EACH resort, NOT a bucketized histo!\n",
    "#in this case, when just looking at one state, they bucketized in groups of 1's or 2's\n",
    "plot_compare('AdultWeekend', 'Adult weekend ticket price ($) - Montana only', state='Montana')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ouch. so yeah, we saw this earlier - BM is by far the most expensive in Montana - AND WE'RE TRYNA TELL THEM TO CHARGE\n",
    "#EVEN MORE! and remem, when we compared its features to other ones in Montana it offered WAY LESS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8.2 Vertical drop<a id='5.8.2_Vertical_drop'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_compare('vertical_drop', 'Vertical drop (feet)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ah okay, so yeah you can go about these plots this way too, of doing histogram and bucketizing and doing frequency vs.\n",
    "#value. but i like doing individual plots of each resort and make y-axis the actual value and order it descending\n",
    "\n",
    "#but yeah, okay, vert drop is towards the higher side so it's putting its money where its mouth is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big Mountain is doing well for vertical drop, but there are still quite a few resorts with a greater drop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8.3 Snow making area<a id='5.8.3_Snow_making_area'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_compare('Snow Making_ac', 'Area covered by snow makers (acres)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good job BM! top performer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big Mountain is very high up the league table of snow making area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8.4 Total number of chairs<a id='5.8.4_Total_number_of_chairs'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_compare('total_chairs', 'Total number of chairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def on the high end of the normal range. beyond this are freak outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so now we're starting to get an idea of why it scored so high / why it was told to charge such a premium--\n",
    "#cuz it did so well in the most important categories!!\n",
    "#that's what we're doing here! we saw what categories were important and we made a function to quickly see how well\n",
    "#BM did in all of them!\n",
    "#I'm sure we could set it up so that we can just make a list of these like 8 features and define a plot grid of like\n",
    "#4x2 and pass the list to it to make one plot for each feature\n",
    "\n",
    "#also though, since Montana is a tough league/division, we should see how the REST OF MONTANA did w/ price predictions\n",
    "#based on their performance in important categories / weight scores / score weights!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big Mountain has amongst the highest number of total chairs, resorts with more appear to be outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8.5 Fast quads<a id='5.8.5_Fast_quads'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_compare('fastQuads', 'Number of fast quads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay that's good, def ahead of most\n",
    "\n",
    "# and remem, the weights are in order as laid out earlier, which we found in unit 4 w/ the COEFS!\n",
    "# remem, these coeffs are according to the LINEAR REGRESSION model! the random forest model ranking IMPORTANCE / \n",
    "# best estimators are basically the same features but in a different order/weight proportions (on the right side)\n",
    "# and note the one we're going w/, for some reason, based on the list/order they provided above and reproduced here\n",
    "# on the left below, is based on LINEAR REGRESSION, although i thought RANDOM FOREST/rf was better and that's what we\n",
    "# went w/ in the end as our 'best model'. but i guess since the top features, regardless of order, were essentially the\n",
    "# same, it doesn't matter so much, the point here is to simply explore the histogram/distribution / where-BM-stands\n",
    "# in the ranks amongst the population\n",
    "\n",
    "# LINEAR REGRESSION/lr                   # RANDOM FOREST/rf (NOTE: All feature importances together add up to 1!!)\n",
    "# vertical_drop        10.767857         # fast_Quads .26\n",
    "# Snow Making_ac        6.290074         # Runs       .25\n",
    "# total_chairs          5.794156         # Snow Making.11\n",
    "# fastQuads             5.745626         # vert drop  .09\n",
    "# Runs                  5.370555         # skiable ter.03\n",
    "# LongestRun_mi         0.181814         # tot chairs .02\n",
    "# trams                -4.142024         # days open. .02\n",
    "# SkiableTerrain_ac    -5.249780         # daysopstrat.02\n",
    "\n",
    "#okay, i guess i could see why the lr ones are better to go w/, cuz of end\n",
    "#but at same time, what's up w/ the negatives that early on??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most resorts have no fast quads. Big Mountain has 3, which puts it high up that league table. There are some values  much higher, but they are rare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oh yeah wow just realized most don't have any! that's what that big bar is - it's not 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8.6 Runs<a id='5.8.6_Runs'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_compare('Runs', 'Total number of runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nicely done BM, nicely done. top-tier once again\n",
    "\n",
    "\n",
    "#btw, would be nice if it said / marked the actual number that it was highlighting for BM value\n",
    "#cuz can't always exactly tell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big Mountain compares well for the number of runs. There are some resorts with more, but not many."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8.7 Longest run<a id='5.8.7_Longest_run'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_compare('LongestRun_mi', 'Longest run length (miles)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wow, again! it seems to have a similar place in all of these, at the top of the normal range, just before the mind freaks /\n",
    "#at the very bottom/start of the insane freaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big Mountain has one of the longest runs. Although it is just over half the length of the longest, the longer ones are rare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8.8 Trams<a id='5.8.8_Trams'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_compare('trams', 'Number of trams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no prob! you're def not alone! that's the overwhelming norm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vast majority of resorts, such as Big Mountain, have no trams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8.9 Skiable terrain area<a id='5.8.9_Skiable_terrain_area'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_compare('SkiableTerrain_ac', 'Skiable terrain area (acres)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED NOW & MATCHES -- totally forgot about the Silverton Mountain Colorado correction!! made a huge diff!! that one mistake alone was what threw off my r2 score completely in part 3 and sent me on a / led me down a wild goose chase!!! unresolved until now / until this tipped me off!! instantly shrunk / transformed the preload!\n",
    "\n",
    "# >>! ski_data.loc[ski_data.Name=='Silverton Mountain', 'SkiableTerrain_ac'] = 1819 !\n",
    "\n",
    "#-- but prior:\n",
    "\n",
    "#interesting - so mine includes the one freak w/ the crazy amount of skiable terrain in Colorado, one that we investigated\n",
    "#in the very beginning and decided to keep. but the template doesn't have it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sveeet!! right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ski_data.copy()[['state','SkiableTerrain_ac']].sort_values(by='SkiableTerrain_ac', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yep, confirmed, they took out that one resort. but they def decided to keep it in and keep it that huge high value\n",
    "# in the beginning, soooerrrr...\n",
    "\n",
    "# ohhhh okay, so went back and looked. so, yes, we DID decide to keep that resort, **BUT**, we DID change it to a\n",
    "# much lower value!!! upon research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yeah so BM is wayyy, way up there!! in the big sky! near the top of the big mountain of graph!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big Mountain is amongst the resorts with the largest amount of skiable terrain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.9 Modeling scenarios<a id='5.9_Modeling_scenarios'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big Mountain Resort has been reviewing potential scenarios for either cutting costs or increasing revenue (from ticket prices). Ticket price is not determined by any set of parameters; the resort is free to set whatever price it likes. However, the resort operates within a market where people pay more for certain facilities, and less for others. Being able to sense how facilities support a given ticket price is valuable business intelligence. This is where the utility of our model comes in.\n",
    "\n",
    "The business has shortlisted some options:\n",
    "1. Permanently closing down up to 10 of the least used runs. This doesn't impact any other resort statistics.\n",
    "2. Increase the vertical drop by adding a run to a point 150 feet lower down but requiring the installation of an additional chair lift to bring skiers back up, without additional snow making coverage\n",
    "3. Same as number 2, but adding 2 acres of snow making cover\n",
    "4. Increase the longest run by 0.2 mile to boast 3.5 miles length, requiring an additional snow making coverage of 4 acres\n",
    "\n",
    "The expected number of visitors over the season is 350,000 and, on average, visitors ski for five days. Assume the provided data includes the additional lift that Big Mountain recently installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay and it has about 100 runs\n",
    "# okay so they'd increase the vert by increasing the LOW POINT / DEPTH / going DOWN, but then this'd be ADDING a run\n",
    "# and also they'd need to add a lift to get ppl back up to ground/sea level. and this would not add to the snow making coverage?\n",
    "# okay #3 is the same option as above but WOULD add partial / make some of it covered w/ snow\n",
    "# increasing the longest run by .2 miles, but that doesn't seem like that would really make it jump by any leaps or bounds\n",
    "# by any means. i mean it's at an interesting position where it's already near the top, but not in the most elite of\n",
    "# elite categories, so there's this gap / deadspace b/w/ it's tier and the superperformers. Getting to 3.5 would put it\n",
    "# at the top of its current class, but it still wouldn't cross the bridge/bridge the gap into the next territory\n",
    "# so it's a toss-up. and that added snow making acreage that would come of it wouldn't really make a dent in the overall\n",
    "# distribution picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_visitors = 350_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oh wow that's a lot of visitors. really? they stay 5 days? okay i guess makes sense, cuz they're going all that way\n",
    "#out to the middle of nowhere. and okay, we're saying that the data we have includes that they already\n",
    "#installed that extra lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay cool so this is grouping all 8 key / most important features in 1\n",
    "all_feats = ['vertical_drop', 'Snow Making_ac', 'total_chairs', 'fastQuads', \n",
    "             'Runs', 'LongestRun_mi', 'trams', 'SkiableTerrain_ac']\n",
    "big_mountain[all_feats]\n",
    "\n",
    "#Thus this^ is gonna give us just these 8 key features of big mountain\n",
    "#nice view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code task 2#\n",
    "#In this function, copy the Big Mountain data into a new data frame\n",
    "#(Note we use .copy()!)   #>>Yes!! gotta make sure we do that! otherwise it's gonna yell at you\n",
    "\n",
    "#And then for each feature, and each of its deltas (changes from the original),\n",
    "\n",
    "#create the modified scenario dataframe (bm2) and make a ticket price prediction\n",
    "\n",
    "#for it. The difference between the scenario's prediction and the current\n",
    "\n",
    "#prediction is then calculated and returned.\n",
    "\n",
    "#Complete the code to increment each feature by the associated delta\n",
    "\n",
    "def predict_increase(features, deltas):\n",
    "    \n",
    "    #ohh okay, so WE'RE specifying the deltas, i.e. manually changing the values \n",
    "    \"\"\"Increase in modelled ticket price by applying delta to feature.\n",
    "    \n",
    "    Arguments:\n",
    "    features - list, names of the features in the ski_data dataframe to change\n",
    "    deltas - list, the amounts by which to increase the values of the features\n",
    "    \n",
    "    Outputs:\n",
    "    Amount of increase in the predicted ticket price\n",
    "    \"\"\"\n",
    "    \n",
    "    bm2 = X_bm.copy()   #okay this creates a copy of X_bm, which are the predicting features of BM, so doesn't include price\n",
    "    \n",
    "    #okay, so this is taking two things - features & deltas. so each item in features is 'f' and each item in\n",
    "    #deltas is 'd'\n",
    "    #so yeah, those are the two things you're feeding it from up there in the function call\n",
    "    \n",
    "    #so remember, zip() takes 2 lists of equal length and makes them into tuple pairs, haha it's like a 'ZIPPER' that\n",
    "    #takes the two independent equal-length lines and 'zips' them together into one unified! it's like each nook is\n",
    "    #paired w/ another on the other side buddy-buddy!\n",
    "    \n",
    "    for f, d in zip(features, deltas): #oh i see so we use zip so that it can go thru two lists at once and / but\n",
    "        bm2[f] += d                   #we can reference them independently w/ two diff variables\n",
    "    return model.predict(bm2).item() - model.predict(X_bm).item()\n",
    "\n",
    "\n",
    "\n",
    "# okay so this is going thru the set of features in features, one by one, 'f by f', and adding the delta to it,\n",
    "# each delta d in the list delta, which we provide\n",
    "\n",
    "#so this finishes going through each f,d pair, adjusting each f feature by each corresponding d delta\n",
    "\n",
    "#so the output is gona be the NEW PREDICTED PRICE based on these new / delta'd variables!!\n",
    "\n",
    "#oh actually, wait - CAREFUL - it's not giving you the price ITSELF - it would if it stopped at / was just:\n",
    "# return model.predict(bm2).item()\n",
    "\n",
    "#but it's subtracting the ORIGINAL predicted price from it\n",
    "\n",
    "#so that means what it's giving you is the PRICE PREDICTION DELTA!!!!!\n",
    "\n",
    "#i.e. what this is showing you is what delta do you get in price for your delta(s) in the other variables????\n",
    "\n",
    "#cuz remem the whole point is to see how different the price looks based on changes in variables,\n",
    "#so we wanna see what the model is most sensitive to\n",
    "#it should be in line w/ the RANDOM FOREST chart we made above/earlier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9.1 Scenario 1<a id='5.9.1_Scenario_1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close up to 10 of the least used runs. The number of runs is the only parameter varying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay so in this scenario, the ONLY feature we're changing is the number of runs, so that's gonna go down by -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in range(-1, -11, -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_delta = [i for i in range(-1, -11, -1)]  #so we're diminishing the runs by 1 down at a time till we're 10 down from original\n",
    "price_deltas = [predict_increase(['Runs'], [delta]) for delta in runs_delta]\n",
    "\n",
    "\n",
    "#okay so remem 'predict_increase' is our custom/homebrew function/homemade/handmade\n",
    "#where we take (features, deltas) as the arguments\n",
    "#so here they're giving us ['Runs'] as the only feature\n",
    "#and [delta], which refers to each item at a time in the range function from -1 to -10\n",
    "#so remem it's bm2[f] += d, so it's gonna 'add -1:-10' aka subtract, so we're good/straight there\n",
    "\n",
    "#so since the features arg is just ONE feature with ONE value, it's just gonna stay constant that value, it's as if\n",
    "#it converts that to a list of itself / duplicates it/copies/drags down\n",
    "\n",
    "#and so since it's iterating thru the range, and we're only interested in the last value, we're only gonna focus on /\n",
    "#care about the last value in the list!\n",
    "\n",
    "\n",
    "#but, in response to that - my question is - wasn't it a lot simpler? like couldn't we have straight just made delta = -10?????\n",
    "#just like how we're holding feature constant @/for Runs????\n",
    "\n",
    "#i guess one thing is this let's us keep it a consistent way for each scenario, i.e. use the same/one function\n",
    "#and the second thing is it let's us see the gradual change / trend over 'time' / w/ each addl unit change\n",
    "\n",
    "#we'll test it out next\n",
    "\n",
    "#and remember, these are price DELTAS!!!! NOT the new predicted prices themselves!! cuz we wanna clearly / right away\n",
    "#see immediately what's happening / how the original predicted price is being affected / what's its relationship to\n",
    "#changes in this/these other variables -- is it going down (like when they're going down)\n",
    "\n",
    "#remember that the delta formula is new_prediction - old_/original_prediction,\n",
    "#so if that delta is POSITIVE+, that means the predicted price is INCREASING in response!\n",
    "#and if the delta is NEGATIVE-, that means the predicted price is DECREASING/going down in response!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#so here's how the price predictions are being affected by changes in the #of runs / how the new price predictions\n",
    "#are/change in response to changes/(decreases) in runs\n",
    "\n",
    "price_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_increase(['Runs'], [-10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DICE!!!!! you were right! you did it!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay so there you have it, it looks like Runs is a very insensitive field! even after dropping the full ***10*** runs\n",
    "# the predicted price only went down $1.81!!!\n",
    "\n",
    "# it seems to work by/on step-changes. notice the 1st number in the set is Zero/0/jeero - that's for ONE closed run!\n",
    "# aka closing down ONE run shouldn't have ANY effect on the price!! and similarly you ca see other steps in the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Code task 3#\n",
    "#Create two plots, side by side, for the predicted ticket price change (delta) for each\n",
    "#condition (number of runs closed) in the scenario and the associated predicted revenue\n",
    "#change on the assumption that each of the expected visitors buys 5 tickets   #>>cuz 5 day stays each!!!\n",
    "#There are two things to do here:\n",
    "#1 - use a list comprehension to create a list of the number of runs closed from `runs_delta`\n",
    "#2 - use a list comprehension to create a list of predicted revenue changes from `price_deltas`\n",
    "runs_closed = [-1 * d for d in runs_delta] #1 #this is simply negating the / each number in runs_delta cuz now we're talking about the NUMBER of runs closed, so need a positive number, not a mathematical operator\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5)) # >> this is that side-by-side plot they want, a '1x2' configuration!!!\n",
    "fig.subplots_adjust(wspace=0.5)\n",
    "\n",
    "#okay so this is for the first plot (ax[0] out of [0] & [1])\n",
    "#we're plotting: x: runs_closed - 1:10  vs.  y: price_deltas\n",
    "#so we're gonna show how price is changed/affected by changes in runs (decreases)\n",
    "#price deltas we're simply borrowing/reusing/recycling directly from the original, as-is\n",
    "\n",
    "ax[0].plot(runs_closed, price_deltas, 'o-')\n",
    "ax[0].set(xlabel='Runs closed', ylabel='Change ($)', title='Ticket price')\n",
    "\n",
    "\n",
    "\n",
    "#okay and for the second plot, we're tryna show the predicted REVENUE CHANGE!\n",
    "#so we're assuming that the number of visitors holds constant throughout all these @ 350,000\n",
    "\n",
    "#okay, so what this is gonna give us is the amount of revenue per each lift loss amt!\n",
    "\n",
    "#so 5 days/visitor * 350,000 visitors * price delta\n",
    "#so this is gonna tell us for each run closed, how much the total REVENUE changes!\n",
    "#we know how much the TICKET PRICE changes, so the total revenue for the season is just the total number of visitors\n",
    "#per the season * the number of days each visitor will stay there on average * price per ticket per/each day!\n",
    "#and so likewise the revenue DELTA will be the total number of visitors per the season * the number of days\n",
    "#each visitor will stay there on average * PRICE DELTA per ticket per/each day!\n",
    "#and so we have these price deltas @ each add'l closed run\n",
    "\n",
    "#okay so it's gonna iterate/list comprehension thru / for each ticket price delta, which is based on, in order, each\n",
    "#closed run, so thus, it's already in order, and so when we do the progressive plot of revenue delta over the number\n",
    "#of runs closed, it's already tied to / in the right order to the runs closed -> the corresponding price delta!!!\n",
    "\n",
    "\n",
    "revenue_deltas = [5 * expected_visitors * p for p in price_deltas] #2\n",
    "\n",
    "ax[1].plot(runs_closed, revenue_deltas, 'o-')\n",
    "ax[1].set(xlabel='Runs closed', ylabel='Change ($)', title='Revenue');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#btw, to give this context, in absolute numbers, since this can be misleading since it's ONLY talking about deltas:\n",
    "#@ 350,000 visitors, for 5 days each, @ $96/day, that's $168 MILLION IN REVENUE! so you might've saw this and thought,\n",
    "#oh man, i thought we were only losing $1.81 a ticket, but now we're losing $3M+!!!\n",
    "\n",
    "#but think about it. that's what we expected/knew - that comes from: $1.81 * 350,000 * 5 = $3.1M+ loss. BUT - now we\n",
    "#know how much we're actually making! $168M!!! so even if we lose $3M / $168M, that's still only 1.8%!!!!\n",
    "#and will prob have a bunch of savings in / from other ways!!!\n",
    "\n",
    "#and notice that it does make sense that the two graphs will be the exact same shape cuz they're based on the same thing\n",
    "#the left is simply at the individual scale and the right is the total\n",
    "#so the right is simply the left multiplied by a CONSTANT COEFFICIENT (5 * 350,000), (i.e. the number of tickets per\n",
    "#season!!!) / THE SAME COEFFICIENT AT EACH POINT!!!\n",
    "\n",
    "#like we showed above: @Step 1, across both:  Left: -$1.81 <> Right: $1.81 * 5 * 350,000 = -$3.1M!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model says closing one run makes no difference. Closing 2 and 3 successively reduces support for ticket price and so revenue. If Big Mountain closes down 3 runs, it seems they may as well close down 4 or 5 as there's no further loss in ticket price. Increasing the closures down to 6 or more leads to a large drop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesting point - if gonna close 3, might as well close 5. would have to weigh diff things. would #runs affect\n",
    "# capacity? if so, would it be better to keep capacity smaller so you can charge more per ticket (throttle supply)\n",
    "# or would more capacity be more advantageous cuz then could bring in more people and sell *MORE* tickets?\n",
    "# but they did say specifically that changing the number of runs WOULD NOT affect any other parameter, like skiable\n",
    "# acres etc, so i guess we'll go w/ that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9.2 Scenario 2<a id='5.9.2_Scenario_2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario, Big Mountain is adding a run, increasing the vertical drop by 150 feet, and installing an additional chair lift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay so this is the one about increasing the vert drop by 150 feet by going / expanding that much LOWER, and this\n",
    "#requires adding another run to do that AS WELL AS another lift - to bring em back up!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code task 4#\n",
    "#Call `predict_increase` with a list of the features 'Runs', 'vertical_drop', and 'total_chairs'\n",
    "#and associated deltas of 1, 150, and 1\n",
    "\n",
    "#oh i just realized we call it \"predict increase\" but it could be that the predicted price is going DOWN! which is\n",
    "#exactly what happened in the last scenario!! but really it's the price CHANGE aka price DELTA/delter!\n",
    "\n",
    "\n",
    "#predict the price increase (CHANGE 'TRIANGLE' DELTA!) based on the following changes to / in the following features\n",
    "ticket2_increase = predict_increase(['Runs', 'vertical_drop', 'total_chairs'], [1, 150, 1])\n",
    "#okay cool so it's a change of +1 run, +150 ft vertical drop, and +1 chair!!\n",
    "\n",
    "\n",
    "revenue2_increase = 5 * expected_visitors * ticket2_increase\n",
    "\n",
    "# so the revenue -- again, *CHANGE/DELTA* -- for the season is gonna be 5 * 350,000 expected visitors * the change \n",
    "# per each ticket, i.e. the thing we just calculated right before that --> ticket2_increase!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'This scenario increases support for ticket price by ${ticket2_increase:.2f}')\n",
    "print(f'Over the season, this could be expected to amount to ${revenue2_increase:.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9.3 Scenario 3<a id='5.9.3_Scenario_3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario, you are repeating the previous one but adding 2 acres of snow making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code task 5#\n",
    "#Repeat scenario 2 conditions, but add an increase of 2 to `Snow Making_ac`\n",
    "ticket3_increase = predict_increase(['Runs', 'vertical_drop', 'total_chairs', ___], [1, 150, 1, ___])\n",
    "revenue3_increase = 5 * expected_visitors * ticket3_increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'This scenario increases support for ticket price by ${ticket3_increase:.2f}')\n",
    "print(f'Over the season, this could be expected to amount to ${revenue3_increase:.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a small increase in the snow making area makes no difference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9.4 Scenario 4<a id='5.9.4_Scenario_4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scenario calls for increasing the longest run by .2 miles and guaranteeing its snow coverage by adding 4 acres of snow making capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code task 6#\n",
    "#Predict the increase from adding 0.2 miles to `LongestRun_mi` and 4 to `Snow Making_ac`\n",
    "predict_increase([___, ___], [___, ___])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No difference whatsoever. Although the longest run feature was used in the linear model, the random forest model (the one we chose because of its better performance) only has longest run way down in the feature importance list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.10 Summary<a id='5.10_Summary'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: 1** Write a summary of the results of modeling these scenarios. Start by starting the current position; how much does Big Mountain currently charge? What does your modelling suggest for a ticket price that could be supported in the marketplace by Big Mountain's facilities? How would you approach suggesting such a change to the business leadership? Discuss the additional operating cost of the new chair lift per ticket (on the basis of each visitor on average buying 5 day tickets) in the context of raising prices to cover this. For future improvements, state which, if any, of the modeled scenarios you'd recommend for further consideration. Suggest how the business might test, and progress, with any run closures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A: 1** Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.11 Further work<a id='5.11_Further_work'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: 2** What next? Highlight any deficiencies in the data that hampered or limited this work. The only price data in our dataset were ticket prices. You were provided with information about the additional operating cost of the new chair lift, but what other cost information would be useful? Big Mountain was already fairly high on some of the league charts of facilities offered, but why was its modeled price so much higher than its current price? Would this mismatch come as a surprise to the business executives? How would you find out? Assuming the business leaders felt this model was useful, how would the business make use of it? Would you expect them to come to you every time they wanted to test a new combination of parameters in a scenario? We hope you would have better things to do, so how might this model be made available for business analysts to use and explore?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A: 2** Your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
